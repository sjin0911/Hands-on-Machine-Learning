# 심층 신경망 훈련

- 심층 신경망을 훈련할 때 발생할 수 있는 문제점
    - 심층 신경망의 출력 층에서 멀어질수록 그레이디언트가 점점 더 작아지거나 커지눈 문제
    - 대규모 신경망을 위한 훈련 데이터가 충분하지 않거나 레이블을 만드는 작업에 비용이 너무 많이 들어감
    - 훈련이 극단적으로 느려짐
    - 훈련 세트에 과대적합 될 위험이 커짐 (샘플이 충분하지 않거나 잡음이 많은 경우)

## 1. 그레이디언트 소실과 폭주 문제

- 그레이디언트 소실 (Vanishing Gradient)
    
    알고리즘이 하위 층으로 진행될수록 그레이디언트가 점점 작아지고 하위 층의 연결 가중치를 변경되지 않은 채로 둘 경우 훈련이 좋은 솔루션으로 수렴되지 않는 문제
    
- 그레이디언트 폭주 (Exploding Gradient)
    
    그레이디언트가 점점 커져서 여러 층이 비정상적으로 큰 가중치로 갱신될 경우 알고리즘이 발산하는 문제
    
    순환 신경망에서 주로 나타남
    
- 문제 발생 원인
    - 로지스틱 시그모이드 활성화 함수와 가중치 초기화 방식의 조합
        
        각 층에서 출력의 분산이 입력의 분산보다 더 커 신경망의 위쪽으로 갈 수록 층을 지날 때마다 분산이 계속 커져 가장 높은 층에서 활성화 함수가 0이나 1로 수렴
        

### 글로럿과 He 초기화

- 예측을 할 때는 정방향으로, 그레이디언트를 역전파할 때는 역방향으로 양방향 신호가 적절하게 흘러야 함
    - 신호가 적절히 흐르기 위해서는
        
        각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 함
        
        역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야 함
        
        → 층의 입력(팬-인, fan-in)과 출력 연결 개수(팬-아웃, fan-out)이 다르다면 보장하기 어려움
        
- 글로럿 초기화(Glorot Initialization) 또는 세이비어 초기화(Xavier Initialization)
    
    각 층의 연결 가중치를 정해진 방식으로 랜덤하게 초기화하는 방법
    
    이 때 $fan_{avg} = (fan_{in}+fan_{out})/2$
    
    - 평균이 0이고 분산이 $\sigma^2 = {1 \over fan_{avg}}$인 정규 분포
    - $r=\sqrt{3 \over fan_{avg}}$일 때 -r과 r 사이의 균등 분포
- 르쿤 초기 (LeCun Initialization)
    
    글로럿 초기화의  $fan_{avg}$를 $fan_{in}$으로 바꾼 버전
    
- He 초기화(He Initialization) 또는 카이밍 초기화(Kaiming Initialization)
    
    ReLU 활성화 함수와 그 변형을 위한 초기화 전략
    

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/65d0ea03-87ac-4b66-a05e-00e9ac55deda/image.png)

### 고급 활성화 함수

- 죽은 ReLU (Dying ReLU)
    
    훈련하는 도안 일부 뉴런이 0 이외의 값을 출력하지 않을 경우 뉴런이 죽었다고 함
    
    큰 학습률을 사용하면 신경망의 뉴런 절반이 죽어있기도함
    
    뉴런의 가중치가 바뀌어 훈련 세트에 있는 모든 샘플에 대해 ReLU 함수의 입력(뉴런 입력의 가중치 합 + 편향)이 음수(ReLU 함수의 그레이디언트가 0이므로 경사 하강법이 더이상 작동하지 않음)가 될 경우 뉴런이 죽음
    
- LeakyReLU (ReLU보다 항상 성능이 높음)
    
    $$
    LeakyReLU_\alpha(z)= max(\alpha z, z)
    $$
    
    하이퍼파라미터 $\alpha$가 이 함수의 ‘새는(Leaky)’ 정도를 결정
    
    - 새는 정도: z<0일 때 이 함수의 기울기
        
        뉴런이 혼수 상태에 오래 있을 수는 있지만 다시 깨어날 가능성을 심어 줌
        
    - RReLU(Randomized Leaky ReLU)
        
        $\alpha$를 랜덤으로 선택하고 테스트 시에는 평균을 사용
        
    - PReLU(Parametric Leaky ReLU)
        
        $\alpha$가 훈련하는 동안 학습되는 방법
        
        다른 모델 파라미터와 마찬가지로 역전파에 의해 변경
        
- ELU와 SELU
    
    ReLU 활성화 함수늬 부드러운 변형
    
    - 문제
        
        RReLU, LeakyReLU, PReLU는 모두 z=0에서 도함수가 갑자기 바뀜
        
        → 이러한 불연속성은 경사 하강법을 최적점에서 진동하게 만들거나 수렴을 느리게 함
        
    - ELU (Exponential Linear Unit)
        
        $$
        ELU_\alpha (z) = \begin{cases}
        \alpha(exp(z)-1) && z<0 \\
        z && z>=0 \end{cases}
        $$
        
        - z<0 일 때 음숫값이므로 활성화 함수의 평균 출력이 0에 더 가까워짐
            
            하이퍼파라미터 $\alpha$는 z가 큰 음숫값일 때 ELU가 수혐할 값의 역수를 정의(보통 1)
            
            → 그레이디언트 소실 문제 완화
            
        - z<0이어도 그레이디언트가 0이 아니므로 뉴런이 죽지 않음
        - $\alpha$=1이면 함수는 매끄러워 경사 하강법의 속도를 높여줌
        - 단점
            
            지수 함수를 사용하므로 ReLU나 그 변형들보다 계산이 느림 (수렴 속도는 빠름)
            
    - SELU (Scaled ELU)
        
        스케일이 조정된 ELU 활성화 함수의 변형
        
        - 완전 연결 층만 쌓아 신경망을 만들고 모든 은닉 층이 SELU 활성화 함수를 사용한다면 네트워크가 자기 정규화(self-normalize)됨
            
            훈련하는 동안 각 층의 출력이 평균 0과 표준 편차 1을 유지하는 경향이 있음
            
            → 그레이디언트 소실과 폭주 문제를 막는 방법
            
        - MLP 중에서도 아주 깊은 네트워크에서 다른 활성화 함수보다 높은 성능을 냄
        - 자기 정규화가 일어날 조건
            - 입력 특성은 반드시 표준화(평균 0, 표준 편차 1)되어야 함
            - 모든 은닉 층의 가중치는 르쿤 정규 분포 초기화로 초기화되어야 함 (kernel_initializer=”lecun_normal”)
            - 자기 정규화는 일반적인 MLP에서만 보장
                
                (스킵 연결(skip connection, 와이드&딥에서 건너뛰어 연결된 층)이나 다른 구조에서는 ELU보다 성능이 떨어짐)
                
            - $l_1, l_2$ 규제, 맥스-노름, 배치 정규화, 드롭아웃과 같은 규제 사용 불가능
- GELU, Swish, Mish
    - GELU
        
        ReLU 활성화 함수의 부드러운 변형
        
        $$
        GELU(z)=z\Phi (z)
        $$
        
        - $\Phi$: 가우스 누적 분포 함수(Cumulative Distribution Function)
        - $\Phi(z)$: 평균이 0이고 분산이 1인 정규 분포에서 랜덤하게 샘플링한 값이 z보다 작을 확률
        
        입력 z가 큰 음수일 때 0에 가까워지고 z가 큰 양수일 때 z에 가까워짐
        
        - 왼쪽에서 직선으로 시작해 아래로 구부어지다가 저점에 도달하고 오른쪽 위로 직진 (지금까지의 활성화 함수는 볼록(convex)한 단조(monotonic) 함수)
            
            모든 위치에서 곡률이 존재 
            
            → 복잡한 작업에서 함수가 잘 작동
            
        - 계산량이 많음
        - $\sigma$가 시그모이드 함수일 때 $z\sigma(1.702 z)$와 거의 동일