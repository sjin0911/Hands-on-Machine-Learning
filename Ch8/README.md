# 차원 축소

차원의 저주: 머신 러닝 훈련 샘플의 매우 많은 특성은 훈련을 느리게 하고 좋은 솔루션을 찾기 어렵게 만듦

- 장점
    - 훈련 속도를 높임
    - 데이터 시각화에 유용
    - 고차원 훈련 세트를 하나의 압축된 그래프로 그릴 수 있고 시각적인 패턴 감지 가능

## 1. 차원의 저주

- 고차원 데이터셋은 매우 희박
    
    대부부의 훈련 데이터가 서로 멀리 떨어져 있음
    
- 예측을 위해 훨씬 많은 외삽(extrapolation)을 해야하기 때문에 예측이 불안정
    
    훈련 세트의 차원이 클수록 과대적합 위험이 커짐
    
- 차원의 저주를 해결하는 방법 중 하나는
    
    훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키움
    
    → 훈련 세트가 기하급수적으로 커짐
    

## 2. 차원 축소를 위한 접근법

### 투영

모든 훈련 샘플은 고차원 공간 안의 저차원 부분 공간에 놓여 있음

→ 서로 강하게 연관된 특성 존재

- 예시
    
    3차원 데이터셋에 모든 훈련 샘플이 거의 평면 형태로 놓여 있음 
    
    모든 훈련 샘플을 이 평면 형태 공간에 수직으로 투영하면 2차원 데이터셋을 얻을 수 있음
    
- 스위스 롤처럼 부분 공간이 뒤틀리거나 휘어 있기 쉬움
    
    스위스 롤의 층이 뭉개질 수 있기 때문에 스위스 롤을 펼쳐서 투영을 해야 함
    

### 매니폴드 학습

d차원 매니폴드는 국부적으로 d차원 초평면(hyperplane)으로 보일 수 있는 n차원 공간의 일부 (d<n)

훈련 샘플이 놓여 있는 매니폴드를 모델링하는 방법

- 2D 매니폴드: 고차원 공간에서 휘어지거나 뒤틀린 2D 모양
    
    스위스 롤의 경우 d=2이고 n=3 (국부적으로 2D 평면으로 보임)
    
- 가정
    - 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 매니폴드 가정(매니폴드 가설)
        - 예시: MNIST 데이터셋의 자유도는 다른 아무 이미지의 자유도보다 낮음
    - 처리해야 할 작업이 저차원의 매니폴드 공간에 표현되면 더 간단해짐
    
    → 항상 유효하지는 않음
    

## 3. 주성분 분석(PCA)

데이터에 가장 가까운 초평면을 정의하고 데이터를 이 평면에 투영

### 분산 보존

올바른 초평면을 선택하기

- 분산이 최대로 보존되는 축을 선택하는 것이 정보 손실이 가장 적음
- = 원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축

### 주성분

- 과정
    1. 훈련 세트에서 분산이 최대인 축을 찾음
    2. 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾음 
    3. 고차원 데이터셋일수록 더 많은 축을 찾음 → n개의 축
- i번째 축을 i번 째 주성분이라고 부름
- 특잇값 분해(SVD)라는 표준 행렬 분해 기술을 이용
    
    훈련 세트 X를 세 개 행렬의 행렬 곱셈인 $U\sum V^T$로 분해
    
    - 찾고자 하는 모든 주성분의 단위 벡터가 V에 담겨 있음