# 차원 축소

차원의 저주: 머신 러닝 훈련 샘플의 매우 많은 특성은 훈련을 느리게 하고 좋은 솔루션을 찾기 어렵게 만듦

- 장점
    - 훈련 속도를 높임
    - 데이터 시각화에 유용
    - 고차원 훈련 세트를 하나의 압축된 그래프로 그릴 수 있고 시각적인 패턴 감지 가능

## 1. 차원의 저주

- 고차원 데이터셋은 매우 희박
    
    대부부의 훈련 데이터가 서로 멀리 떨어져 있음
    
- 예측을 위해 훨씬 많은 외삽(extrapolation)을 해야하기 때문에 예측이 불안정
    
    훈련 세트의 차원이 클수록 과대적합 위험이 커짐
    
- 차원의 저주를 해결하는 방법 중 하나는
    
    훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키움
    
    → 훈련 세트가 기하급수적으로 커짐
    

## 2. 차원 축소를 위한 접근법

### 투영

모든 훈련 샘플은 고차원 공간 안의 저차원 부분 공간에 놓여 있음

→ 서로 강하게 연관된 특성 존재

- 예시
    
    3차원 데이터셋에 모든 훈련 샘플이 거의 평면 형태로 놓여 있음 
    
    모든 훈련 샘플을 이 평면 형태 공간에 수직으로 투영하면 2차원 데이터셋을 얻을 수 있음
    
- 스위스 롤처럼 부분 공간이 뒤틀리거나 휘어 있기 쉬움
    
    스위스 롤의 층이 뭉개질 수 있기 때문에 스위스 롤을 펼쳐서 투영을 해야 함
    

### 매니폴드 학습

d차원 매니폴드는 국부적으로 d차원 초평면(hyperplane)으로 보일 수 있는 n차원 공간의 일부 (d<n)

훈련 샘플이 놓여 있는 매니폴드를 모델링하는 방법

- 2D 매니폴드: 고차원 공간에서 휘어지거나 뒤틀린 2D 모양
    
    스위스 롤의 경우 d=2이고 n=3 (국부적으로 2D 평면으로 보임)
    
- 가정
    - 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 매니폴드 가정(매니폴드 가설)
        - 예시: MNIST 데이터셋의 자유도는 다른 아무 이미지의 자유도보다 낮음
    - 처리해야 할 작업이 저차원의 매니폴드 공간에 표현되면 더 간단해짐
    
    → 항상 유효하지는 않음
    

## 3. 주성분 분석(PCA)

데이터에 가장 가까운 초평면을 정의하고 데이터를 이 평면에 투영

### 분산 보존

올바른 초평면을 선택하기

- 분산이 최대로 보존되는 축을 선택하는 것이 정보 손실이 가장 적음
- = 원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축

### 주성분

- 과정
    1. 훈련 세트에서 분산이 최대인 축을 찾음
    2. 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾음 
    3. 고차원 데이터셋일수록 더 많은 축을 찾음 → n개의 축
- i번째 축을 i번 째 주성분이라고 부름
- 특잇값 분해(SVD)라는 표준 행렬 분해 기술을 이용
    
    훈련 세트 X를 세 개 행렬의 행렬 곱셈인 $U\sum V^T$로 분해
    
    - 찾고자 하는 모든 주성분의 단위 벡터가 V에 담겨 있음
- PCA는 데이터셋의 평균이 0이라고 가정
    - 파이썬의 PCA 클래스는 이 작업을 대신 처리해 줌

### d차원으로 투영하기

주성분을 모두 추출한 뒤 처음 d개의 주성분으로 정의한 초평면에 투영해 데이터셋의 차원을 d차원으로 축소 가능

- 초평면 훈련 세트를 투영하고 d차원으로 축소된 데이터셋을 얻는 식
    
    $$X_{d-proj}=XW_d$$
    
    - $W_d$: V의 첫 d 열로 구성된 행렬

### 사이킷런 사용하기

사이킷런의 PCA 모델은 SVD를 사용해 구현 

### 설명된 분산의 비율

각 주성분의 축을 따라 있는 데이터셋의 분산 비율

- 사이킷런 PCA에 explained_varience_ratio_ 변수에 저장

### 적절한 차원 수 선택

충분한 분산이 될 때까지 더해야 할 차원의 수를 선택

- 훈련 집합의 분산을 최대한 보존하는데 필요한 차원 수를 계산해 차원 수 선택 (변곡점 존재)
- 지도 학습 작업의 전처리 단계로 차원 축소를 사용하는 경우
    
    다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝 가능
    

### 압축을 위한 PCA

차원 축소 후 훈련 세트는 훨씬 작은 공간을 차지

- 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용해 다시 원래의 차원으로 되돌릴 수 있음
    
    원본 데이터셋과 완전 똑같지는 않지만 비슷한 데이터셋을 얻을 수 있음
    
- 재구성 오차: 원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리
- inverse_transform() 메서드를 사용
    
    $$X_{recovered}=X_{d-proj}W_d^T$$
    

### 랜덤 PCA

확률적 알고리즘을 사용한 처음 d개의 주성분에 대한 근삿값을 찾는 방법

- 완전한 SVD 방식의 시간복잡도: $O(m*n^2)+O(n^3)$
- 랜덤 PCA의 시간복잡도: $O(m*d^2)+O(d^3)$
- max(m,n)>500이고 n_components가 min(m,n)의 80%보다 작은 정수인 경우 사이킷런이 자동으로 랜덤 PCA를 사용
    - svd_solver=”full”로 설정할 경우 완전한 SVD 사용

### 점진적 PCA

훈련 세트를 미니배치로 나눈 뒤 IPCA 알고리즘에 한 번에 하나씩 주입

- 훈련 세트가 매우 클 때 유용
    
    온라인으로(새로운 데이터에 대해 실시간으로) PCA 적용 가능
    

훈련 세트가 매우 클 경우 훈련 속도가 매우 느려짐

## 4. 랜덤 투영

랜덤한 선형 투영을 사용해 데이터를 저차원 공간에 투영

- 실제로 거리를 상당히 잘 보존할 가능성이 매우 높음
- 거리가 주어진 허용 오차 이상으로 변하지 않도록 보장하기 위해 보존할 최소 차원 수를 결정하는 방정식
    
    → johnson_lindenstrauss_min_dim() 함수를 사용
    
- 데이터셋의 크기만 있으면 랜덤한 행렬 생성 가능(데이터셋 자체를 사용하지 않음)
- 사이킷런의 GaussianRandomProjection 클래스를 사용
- 두 번째 랜덤 투영 변환기
    
    랜덤 투영과 동일한 작업을 수행하지만 랜덤 행렬이 희소
    
    → 메모리를 훨씬 적게 사용
    
    - 입력이 희소할 경우 이 변환은 희소성을 유지
    - 규모가 크거나 희박한 데이터셋의 경우 더 유용
    - r(밀도): 희소한 랜덤 행렬에서 0이 아닌 항목의 비율
        
        기본적으로 밀도는 $1/ \sqrt{n}$
        
        density 매개변수를 통해 밀도 설정 가능
        
    - 희소 랜덤 행렬의 각 항목은 0이 아닐 확률 r을 가지고
        
        0이 아닌 각 값은 동일확률로 -v 또는 +v (v= $1/\sqrt{dr}$)
        
    - 역변환: 성분 행렬의 유사역행렬을 계산하고 축소된 데이터에 유사역행렬의 전치를 곱하기

## 5. 지역 선형 임베딩(LLE: Locally Linear Embedding)

비선형 차원 축소(NLDR: Nonlinear Dimensionality Reduction) 기술

- 투영에 의존하지 않는 매니폴드 학습
- 각 훈련 샘플이 최근접 이웃에 얼마나 선형적으로 연관되어 있는지 측정하고
국부적인 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현을 찾음
- 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는데 유용
- LLE의 작동방식
    1. 알고리즘이 각 훈련 샘플 $x^{(i)}$에 대해 k개의 최근접 이웃을 찾음
    2. 이 이웃에 대해 선현 함수로 $x^{(i)}$를 재구성
        
        $$\hat W= \underset{W}{argmin}\sum_{i=1}^m(x^{(i)}-\sum_{j=1}^m w_{i,j}x^{(j)})^2 \\
        [조건] \begin{cases} w_{i,j} & x^{(j)}가 x^{(i)}의 최근접 이웃 k개 중 하나가 아닐 때 \\ \sum_{j=1}^m w_{i,j}=1 & i=1,2,...,m일 때 \end{cases}$$
        
        $x^{(i)}$와 $\sum_{j=1}^m w_{i,j}x^{(j)}$ 사이의 제곱 거리가 최소가 되는 $w_{i,j}$를 찾음
        
        여기서 $x^{(j)}$가 $x^{(i)}$의 가장 가까운 k개의 이웃 중 하나가 아닐 경우 $w_{i,j}$=0
        
        샘플을 고정하고 최적의 가중치를 찾음
        
        ⇒ 제한이 있는 최적화 문제
        
        - 이 단계를 거치며 가중치 행렬 $\hat W$은 훈련 샘플 사이에 있는 지역 선형 관계를 담음
    3. 가능한 한 선형 관계가 보존되도록 훈련 샘플을 d차원 공간으로 매핑
        - 관계를 보존하는 차원 매핑 (제약이 없는 최적화 문제)
            
            $$\hat Z=\underset{z}{argmin}\sum_{i=1}^m(z^{(i)}-\sum_{j=1}^m\hat w_{i,j} x^{(j)})^2$$
            
            $z^{(i)}$: d차원 공간에서 $x^{(i)}$의 상
            
            - 가중치를 고정하고 저차원 공간에서 샘플 이미지의 최적 위치를 찾음
- 계산 복잡도
    - k개의 최근접 이웃을 찾음 → $O(m \log{(m)} n\log{(k)})$
    - 가중치 최적화 → $O(mnk^3)$
    - 저차원 표현을 만듦 → $O(dm^2)$
    
    → 대규모 데이터셋에는 부적절
    
    데이터가 비선형인 경우 훨씬 더 나은 저차원 표현 구성
    

## 6. 다른 차원 축소 기법

- 다차원 스케일링 (MDS: MultiDimensional Scaling)
    
    샘플 간의 거리를 보존하며 차원 축소
    
    - 랜덤 추영은 고차원 데이터에는 적합하지만 저차원 데이터에는 잘 작동하지 않음
- Isomap
    
    각 샘플을 가장 가까운 이웃과 연결하는 방식으로 샘플 간의 지오데식 거리를 유지하며 차원 축소
    
    - 그래프에서 두 노드 사이의 지오데식 거리는 두 노드 사이의 최단 경로를 이루는 노드의 수
- t-SNE
    
    비슷한 샘플은 가까이, 비슷하지 않은 샘플은 멀리 떨어지도록하며 차원축소
    
    - 주로 시각화에 사용
        
        고차원 공간에 있는 샘플의 군집을 시각화할 때 사용
        
- 선형 판별 분석
    
    선형 분류 알고리즘 
    
    - 훈련 과정에서 클래스 사이를 가장 잘 구분하는 축을 학습
        
        데이터가 투영되는 초평면을 정의하는데 사용
        
    - 투영을 통해 가능한 한 클래스를 멀리 떨어지게 유지
        
        다른 분류 알고리즘을 적용하기 전에 차원을 축소시키기 좋음
        

## 연습문제

1. 데이터셋의 차원을 축소해 
    1. 훈련 속도를 높임
    2. 데이터 시각화해 직관적인 정보를 얻을 수 있음
    3. 메모리 공간 절약(압축)
    
    단점은 
    
    1. 데이터의 손실을 막을 수 없음 → 알고리즘의 성능 감소
    2. 계산 비용 높음
    3. 파이프라인의 복잡도 증가
    4. 변환된 데이터를 이해하기 어려움
2. 저차원 공간에는 없는 많은 문제가 고차원 공간에서 발생
    
    과대적합의 위험이 크고 패턴을 찾기 매우 어려움
    
    머신러닝 샘플의 많은 특성은 훈련을 느리게 하고 좋은 솔루션을 찾기 어렵게 만듦
    
3. 원상복구 가능한 경우도 있고 가능하지 않은 경우도 있음
    
    완벽하게 되돌리는 것은 불가능하지만 PCA같은 알고리즘은 원본과 비슷한 데이터셋을 재구성할 수 있는 역변환 방법을 가지고 있음
    
4. 불필요한 차원을 제거하기 때문에 대부분 데이터셋에서 차원 축소에 사용 가능
    
    불필요한 차원이 없는 경우 사용 불가능
    
5. 분산을 최대 95% 보장하는 최소 주성분의 개수가 곧 차원이 됨
6. 기본 PCA는 오래걸리기 때문에 데이터의 크기가 비교적 작을 때 사용
    
    점진적 PCA는 데이터 샘플 수가 매우 많을 때(메모리에 담을 수 없는 대용량 데이터셋) 사용하고 온라인 작업에 사용가능하지만 기본 PCA보다 느림
    
    랜덤 PCA는 데이터가 메모리 크기에 맞고 차원을 크게 축소시킬 때 사용(매우 빠름)
    
    랜덤 투영은 희소 행렬을 사용하기 때문에 규모가 크거나 희박한 데이터 셋일 경우 사용 + 매우 고차원일 경우 
    
7. 직관적으로 데이터셋에서 너무 많은 정보를 잃지 않으면서 차원을 많이 제거할 수 있다면 차원 축소 알고리즘은 잘 작동
    
    측정하기 위해선 역변환을 수행해 재구성 오차를 측정하는 방법
    
    차원 축소 알고리즘을 전처리 과정으로 사용할 경우 두번째 알고리즘의 성능 측정
    
8. 두 개의 축소 알고리즘 연결 가능
    
    대표적인 예시로 PCA나 랜덤 투영으로 불필요한 차원을 제거하고 LLE와 같은 느린 알고리즘을 적용 → LLE만 사용했을 때와 비슷한 결과를 내지만 속도가 매우 빨라짐