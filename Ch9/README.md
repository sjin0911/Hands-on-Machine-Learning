알고리즘이 레이블이 없는 데이터를 사용해 학습하는 방법

## 1. 군집

군집: 비슷한 샘플을 구별해 하나의 클러스터 또는 비슷한 샘플의 그룹으로 할당하는 작업

- 분류와 비슷하게 각 샘플은 하나의 그룹에 할당
- 군집이 사용되는 애플리케이션
    - 고객 분류
    - 데이터 분석
    - 차원 축소 기법
    - 특성 공학
    - 이상치 탐지
    - 준지도 학습
    - 검색 엔진
    - 이미지 분할
- 클러스터의 정의는 상황에 따라 다름 → 종류가 매우 다양

### k-평균

각 클러스터의 중심을 찾고 가장 가까운 클러스터에 샘플을 할당

- 각 샘플은 클러스터 중 하나에 할당
- 각 샘플의 레이블: 알고리즘이 샘플에 할당한 클러스터의 인덱스
    - KMeans의 labels_에 예측 레이블을 저장
- 클러스터의 결정 경계를 그려보면 보로노이 다이어그램을 얻을 수 있음
- 클러스터의 크기가 많이 다르면 잘 작동하지 않음
    
    → 샘플을 클러스터에 할당할 때 센트로이드까지의 거리만 고려하기 때문
    
- 하드 군집: 샘플을 하나의 클러스터에 할당
- 소프트 군집: 클러스터마다 샘플에 점수를 부여
    - 샘플과 센트로이드 사이의 거리나 가우스 방사 기저 함수와 같은 유사도 점수를 사용
    - KMeans 클래스의 transform() 메서드
    - 고차원 데이터셋을 이런 방식으로 k차원 데이터셋으로 변환 가능
- k-평균 알고리즘
    1. 처음에는 센트로이드를 랜덤하게 설정
    2. 샘플에 레이블을 할당하고 센트로이드를 업데이트
    3. 위 과정을 센트로이드에 변화가 없을 때까지 반복
    
    → 제한된 횟수 안에 수렴하는 것을 보장
    
    - 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리는각 단계마다 감소하고 음수가 될 수 없기 때문에 수렴 보장
    - 알고리즘의 수렴은 보장되지만 적절한 솔루션으로의 수렴은 보장하지 못 함
- 센트로이드 초기화 방법
    - 센트로이드의 근사 위치를 저장한 넘파이 배열을 init 매개변수로 넘겨주기
    - 센트로이드 랜덤 초기화하기
        - 랜덤 초기화를 n_init 횟수만큼 반복하고 가장 좋은 솔루션을 선택
        - 이너셔: 각 샘플과 가장 가까운 센트로이드 사이의 제곱 거리 합, 최선의 솔루션 판단할 때 사용
    - k-평균++ 알고리즘
        1. 데이터셋에서 랜덤으로 균등하게 하나의 센트로이드 $c^{(i)}$를 선택
        2. $D(x^{(i)})^2/\sum_{j=1}^mD(x^{(j)})^2$의 확률로 샘플 $x^{(i)}$를 새로운 센트로이드로 ($c^{(i)}$)로 선택 
            
            → $D(x^{(i)})$는 샘플과 이미 선택된 가장 가까운 센트로이드까지의 거리
            
        3. k개의 센트로이드가 선택될 때까지 이전 단계를 반복
        
        KMeans 클래스가 사용하는 초기화 방법
        
- k-평균 속도 개선
    
    클러스터가 많은 일부 대규모 데이터셋에서 불필요한 거리 계산을 피함으로써 알고리즘의 속도 높이기 
    
    - 삼각부등식을 사용
    - 샘플과 센트로이드 사이의 거리를 위한 하한선과 상한선을 유지
- 미니배치 k-평균
    
    각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동시키는 방법
    
    → 알고리즘의 속도를 3배에서 4배 정도 높임
    
    - 일반 k-평균 알고리즘보다 이너셔는 일반적으로 더 나쁨
- 데이터셋이 메모리에 들어가지 않는 방법
    - memmap 클래스 사용
    - MiniBatchKMeans 클래스의 partial_fit()을 사용
- 최적의 클러스터 개수 찾기
    - 이너셔는 일반적으로 k가 증가함에 따라 점점 작아지므로 k를 선택할 때 좋은 성능 지표가 아님
    - 이너셔는 k의 함수로 그래프를 그리면 엘보처럼 굴곡이 있는 지점이 생김 → 이너셔가 빠르게 감소하다 느리게 감소하는 구간
    - 실루엣 점수 사용하기
        - 실루엣 점수: 모든 샘플에 대한 실루엣 계수의 평균
        - 실루엣 계수
            
            $$
            (b-a)/max(b,a)
            $$
            
            a: 동일한 클러스터에 있는 다른 샘플까지 평균 거리
            
            b: 가장 가까운 클러스터까지 평균 거리
            
            -1부터 +1 사이의 값
            
        - 실루엣 다이어그램: 모든 샘플의 실루엣 계수를 할당된 클러스터와 계수 값으로 정렬해 그린 그래프
            
            각 클러스터마다 칼 모양의 그래프를 그림
            
            - 높이: 클러스터가 포함하고 있는 샘플의 개수
            - 너비: 클러스터에 포함된 샘플의 정렬된 실루엣 계수
            - 수직 파선: 각 클러스터 개수에 해당하는 평균 실루엣 점수

### k-평균의 한계

- 장점
    - 속도가 빠르고 확장이 용이
- 단점
    - 최적에 가까운 솔루션을 찾기 위해 여러 번의 반복이 필요
    - 클러스터 개수 지정
    - 크기 또는 밀집도가 서로 다르거나 원형이 아닐 경우 잘 작동하지 않음
        
        → 일반적으로 특성의 스케일을 맞출경우 알고리즘에 도움을 줌
        

### 군집을 사용한 이미지 분할

이미지를 여러 개의 세그먼트로 분할하는 작업

- 색상 분할: 동일한 색상을 가진 픽셀을 같은 세그먼트에 할당
- 시멘틱 분학: 동일한 종류의 물체에 속한 모든 픽셀을 같은 세그먼트에 할당
- 인스턴스 분할: 개별 객체에 속한 모든 픽셀을 같은 세그먼트에 할당

### 군집을 사용한 준지도 학습

레이블이 없는 데이터가 많고 레이블이 있는 데이터가 적을 경우 사용

- 레이블이 있는 데이터에 대해 기준 성능을 얻기
- kmeans 알고리즘 훈련시키고 대표 이미지(센트로이드와 가장 가까운 이미지) 찾기
- 대표 샘플에 수동으로 레이블 할당
- 레이블 전파: 할당한 레이블을 동일한 클러스터에 있는 모든 샘플로 전파
    - sklearn.semi_supervised의 LabelSpreading과 LabelPropagation
- 클러스터 중심에서 가장 먼 1%의 샘플을 무시 → 이상치 제거
    - SelfTrainingClassifier에 기본 분류기를 제공하면 위 과정을 반복
        
        레이블이 지정된 샘플에서 훈련한 다음 이를 사용하여 레이블이 지정되지 않은 샘플의 레이블을 예측 → 가장 확신하는 레이블로 훈련 세트를 업데이트 → 더 이상 레이블을 추가할 수 없을 때까지 반복
        

### 능동학습

전문가가 학습 알고리즘과 상호 작용하여 알고리즘이 요청할 때 특정 샘플의 레이블을 제공

- 불확실성 샘플링을 가장 많이 사용
    1. 지금까지 수집한 레이블된 샘플에서 모델을 훈련
        
        이 모델을 사용해 레이블되지 않은 모든 샘플에 대한 예측을 생성
        
    2. 모델이 가장 불확실하게 예측한 샘플(추정 확률이 낮은 샘플)을 전문가에게 보내 레이블을 붙임
    3. 레이블을 부여하는 노력만큼의 성능이 향상되지 않을 때까지 반복
- 모델을 가장 크게 바꾸는 샘플이나 모델의 검증 점수를 가장 크게 떨어뜨리는 샘플, 여러 개의 모델이 동일한 예측을 내지 않는 샘플에 대한 레이블 요청

### DBSCAN(Density-based Spatial Clustering of Applications with Noise)

밀집된 연속적 지역을 클러스터로 정의

- 알고리즘 작동 방식
    - 알고리즘이 각 샘플에서 작은 거리인 $\epsilon$내에 샘플이 몇 개 놓여있는 지 셈 ($\epsilon$-이웃)
    - $\epsilon$-이웃 내에 적어도 min_samples개 샘플이 있다면 이를 핵심 샘플로 간주
    - 핵심 샘플이 이웃에 있는 모든 샘플은 동일한 클러스터에 속함
        
        이웃에 다른 핵심 샘플이 포함될 수 있으므로 핵심 샘플의 이웃의 이웃은 계속해서 하나의 클러스터 형성
        
    - 핵심 샘플이 아니고 이웃도 아닌 샘플은 이상치로 판단
- 모든 클러스터가 밀집되지 않은 지역과 잘 구분될 때 좋은 성능을 냄
    - 사이킷런의 DBSCAN 클래스
- 장점
    - 하이퍼파라미터가 2개로 매우 간단
    - 클러스터의 모양과 개수에 상관없이 감지
    - 이상치에 안정적
- 단점
    - 클러스터 간의 밀집도가 크게 다르거나 (계층적 DBSCAN 사용 가능)
    - 일부 클러스터 주변에 저밀도 영역이 충분히 없는 경우
    
    알고리즘이 잘 작동하지 않음
    
    - 계산 복잡도가 $O(m^2n)$이므로 대규모 데이터셋에 부적절

### 다른 군집 알고리즘

- 병합 군집
    
    클러스터 계층을 밑바닥부터 위로 쌓아 구성
    
    - 반복마다 인접한 클러스터 쌍을 연결
    - 병합된 클러스터 쌍을 트리로 그려 이진트리 형성 (트리의 리프는 개별 샘플)
    - 장점
        - 다양한 형태의 클러스터 감지 가능
        - 특정 클러스터 개수를 선택하는 데 도움이 되는 클러스터 트리 생성 가능
        - 이웃한 샘플 간의 거리를 담은 희소 행렬을 연결 행렬로 전달하는 방식을 통해 대규모 샘플에 적용 가능
- BIRCH(Balanced Iterative Reducing and Clustering using Hierarchies)
    
    대규모 데이터셋을 위한 알고리즘
    
    - 특성 개수가 너무 많지 않다면(20개 이하) 배치 k-평균보다 빠르고 비슷한 결과 도축
    - 새로운 샘플을 클러스터에 빠르게 할당할 수 있는 정보를 담은 트리 구조를 생성
    - 모든 샘플을 저장하지 않음
- 평균-이동
    - 작동 방법
        - 각 샘플을 중심으로 하는 원을 그림
        - 원마다 안에 포함된 모든 샘플의 평균을 구함
        - 원의 중심을 편규점으로 이동
        - 원이 움직이지 않을 때까지 위 과정을 반복
    - 지역의 최대 밀도를 찾을 때까지 높은 쪽으로 원을 이동
    - 동일한 지역에 안착한 원에 있는 모든 샘플은 동일한 클러스터
    - 장점
        - 모양이나 개수에 상관없이 클러스터를 찾음
        - 하이퍼파라미터 개수가 적음(대역폭 설정 딱 한개)
        - 국부적인 밀집도 추정에 의존
    - 단점
        - 클러스터 내부 밀집도가 불균형할 때 클러스터를 여러 개로 나누는 경향이 있음
        - 시간복잡도가 $O(m^2)$로 대규모 데이터셋에 적합하지 않음
- 유사도 전파
    
    모든 샘플은 자신을 대표할 다른 샘플을 선택할 때까지 샘플간의 메시지를 반복적으로 교환하는 방법
    
    - 선출된 샘플을 예시라고 함
    - 각 예시와 이를 선출한 모든 샘플은 하나의 클러스터 형성
    - 선호도 전파도 비슷하게 작동
    - 장점
        - 클러스터의 중심 근처에 위치한 예시를 선택하는 경향이 있음
        - 미리 클러스터의 수를 정할 필요 없이 훈련 중에 결정
        - 다양한 크기의 클러스터 처리 가능
    - 단점
        - 시간복잡도가 $O(m^2)$로 대규모 데이터셋에 적합하지 않음
- 스펙트럼 군집
    
    샘플 사이의 유사도 행렬을 받아 저차원 임베딩 형성
    
    만들어진 저차원 공간에서 다른 군집 알고리즘을 사용
    
    - 장점
        - 복잡한 클러스터 구조를 감지하고 그래프 컷을 찾는 데 사용 가능
    - 단점
        - 샘플 개수가 많으면 잘 적용되지 않음
        - 클러스터의 크기가 매우 다를 경우 잘 작동하지 않음

## 2. 가우스 혼합(GMM: Gaussian Mixture Model)

샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 분포에서 생성되었다고 가정하는 확률 모델

- 하나의 가우스 분포에서 생성된 모든 샘플은 하나의 클러스터 형성
    
    일반적으로 타원형
    
    - 타원의 모양, 크기, 밀집도, 방향이 모두 다름
- GMM의 변형 GaussianMixture
    
    가장 간단한 버전
    
    - 사전에 가우스 분포의 개수 k를 알아야 함
    - 데이터셋 X가 아래 확률 과정을 통해 생성되었다고 가정
        - 샘플마다 k개의 클러스터에서 랜덤하게 한 클러스터가 선택
            
            j번째 클러스터를 선택할 확률은 클러스터의 가중치 $\phi^{(j)}$
            
            i번째 샘플을 위해 선택한 클러스터 인덱스 $z^{(i)}$
            
        - i번째 샘플이 j번째 클러스터에 할당되었다면($z^{(i)}=j$)
            
            이 샘플의 위치 $x^{(i)}$는 평균이 $\mu^{(j)}$이고 공분산 행렬이 $\sum^{(j)}$인 가우스 분포에서 랜덤하게 샘플링
            
            $x^{(i)}$ ~ $N(\mu^{(j)}, \sum^{(j)})$
            
    - 데이터가 X가 주어지면 가중치 $\phi$, 전체 분포의 파라미터 $\mu^{(i)}$에서 $\mu^{(k)}$까지, $\sum^{(i)}$에서 $\sum^{(k)}$까지 추정
    - 기댓값-최대화(EM: Expectation-Maximization) 알고리즘
        
        클러스터 파라미터를 랜덤하게 초기화하고 수렴할 때까지 두 단계를 반복
        
        - 기댓값 단계: 먼저 샘플을 클러스터에 할당(확률을 예측)
        - 최대화 단계: 클러스터를 업데이트
            
            각 클러스터가 데이터 셋에 있는 모든 샘플을 사용해 업데이트하고 클러스터에 속할 추정 확률로 샘플에 가중치가 적용
            
        
        → k-평균은 하드 클러스터 할당을 사용하지만 EM은 소프트 클러스터 할당을 사용
        
- 생성 모델: 이 모델에서 새로운 샘플 생성 가능
- 주어진 위치에서 모델의 밀도 추정 가능
    
    score_sample() 메서드 반환값의 지숫값 = 샘플의 위치에서 PDF 값
    
- 알고리즘이 학습할 파라미터 개수를 제한
    - coveriance_type 매개변수 활용
        - “spherical”: 모든 클러스터가 원형, 다른 분산을 가짐
        - “diag”: 어떤 타원형도 가능, 타원의 축은 좌표축과 나란 (= 공분산 행렬이 대각 행렬)
        - “tied”: 모든 클러스터가 동일한 타원 모양, 크기, 방향을 가짐 (= 공분산 행렬을 모두 공유)
        - 기본값 “full”: 각 클러스터의 모양, 크기, 방향에 제약이 없음

### 가우스 혼합을 사용한 이상치 탐지

밀도가 낮은 지역에 있는 모든 샘플을 이상치로 볼 수 있음

- 사용할 밀도 임곗값: 일반적인 정밀도/재현율 트레이드오프
- 특이치 탐지: 알고리즘을 이상치로 오염되지 않은 깨끗한 데이터셋에서 훈련

### 클러스터 개수 선택

이너셔나 실루엣 점수를 사용할 수 없음 → 클러스터가 타원형이거나 크기가 다를 때 안정적이지 않음

- BIC(Xayesian Information Criterion)와 AIC(Akaike Information Criterion)
    
    $$
    BIC= \log{(m)}p-2\log{(\hat l)} \\
    AIC=2p-2\log{(\hat l)}
    $$
    
    m: 샘플의 개수
    
    p: 모델이 학습할 파라미터 개수
    
    $\hat l$ : 모델의 가능도 함수(likelihood function)의 최댓값
    
    - 학습할 파라미터가 많은 모델에게 벌칙을 가하고 데이터에 잘 맞는 모델에게 보상을 더함 → 종종 동일한 모델 선택
    - 가능도 함수는 확률 분포가 아님# 비지도 학습

알고리즘이 레이블이 없는 데이터를 사용해 학습하는 방법

## 1. 군집

군집: 비슷한 샘플을 구별해 하나의 클러스터 또는 비슷한 샘플의 그룹으로 할당하는 작업

- 분류와 비슷하게 각 샘플은 하나의 그룹에 할당
- 군집이 사용되는 애플리케이션
    - 고객 분류
    - 데이터 분석
    - 차원 축소 기법
    - 특성 공학
    - 이상치 탐지
    - 준지도 학습
    - 검색 엔진
    - 이미지 분할
- 클러스터의 정의는 상황에 따라 다름 → 종류가 매우 다양

### k-평균

각 클러스터의 중심을 찾고 가장 가까운 클러스터에 샘플을 할당

- 각 샘플은 클러스터 중 하나에 할당
- 각 샘플의 레이블: 알고리즘이 샘플에 할당한 클러스터의 인덱스
    - KMeans의 labels_에 예측 레이블을 저장
- 클러스터의 결정 경계를 그려보면 보로노이 다이어그램을 얻을 수 있음
- 클러스터의 크기가 많이 다르면 잘 작동하지 않음
    
    → 샘플을 클러스터에 할당할 때 센트로이드까지의 거리만 고려하기 때문
    
- 하드 군집: 샘플을 하나의 클러스터에 할당
- 소프트 군집: 클러스터마다 샘플에 점수를 부여
    - 샘플과 센트로이드 사이의 거리나 가우스 방사 기저 함수와 같은 유사도 점수를 사용
    - KMeans 클래스의 transform() 메서드
    - 고차원 데이터셋을 이런 방식으로 k차원 데이터셋으로 변환 가능
- k-평균 알고리즘
    1. 처음에는 센트로이드를 랜덤하게 설정
    2. 샘플에 레이블을 할당하고 센트로이드를 업데이트
    3. 위 과정을 센트로이드에 변화가 없을 때까지 반복
    
    → 제한된 횟수 안에 수렴하는 것을 보장
    
    - 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리는각 단계마다 감소하고 음수가 될 수 없기 때문에 수렴 보장
    - 알고리즘의 수렴은 보장되지만 적절한 솔루션으로의 수렴은 보장하지 못 함
- 센트로이드 초기화 방법
    - 센트로이드의 근사 위치를 저장한 넘파이 배열을 init 매개변수로 넘겨주기
    - 센트로이드 랜덤 초기화하기
        - 랜덤 초기화를 n_init 횟수만큼 반복하고 가장 좋은 솔루션을 선택
        - 이너셔: 각 샘플과 가장 가까운 센트로이드 사이의 제곱 거리 합, 최선의 솔루션 판단할 때 사용
    - k-평균++ 알고리즘
        1. 데이터셋에서 랜덤으로 균등하게 하나의 센트로이드 $c^{(i)}$를 선택
        2. $D(x^{(i)})^2/\sum_{j=1}^mD(x^{(j)})^2$의 확률로 샘플 $x^{(i)}$를 새로운 센트로이드로 ($c^{(i)}$)로 선택 
            
            → $D(x^{(i)})$는 샘플과 이미 선택된 가장 가까운 센트로이드까지의 거리
            
        3. k개의 센트로이드가 선택될 때까지 이전 단계를 반복
        
        KMeans 클래스가 사용하는 초기화 방법
        
- k-평균 속도 개선
    
    클러스터가 많은 일부 대규모 데이터셋에서 불필요한 거리 계산을 피함으로써 알고리즘의 속도 높이기 
    
    - 삼각부등식을 사용
    - 샘플과 센트로이드 사이의 거리를 위한 하한선과 상한선을 유지
- 미니배치 k-평균
    
    각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동시키는 방법
    
    → 알고리즘의 속도를 3배에서 4배 정도 높임
    
    - 일반 k-평균 알고리즘보다 이너셔는 일반적으로 더 나쁨
- 데이터셋이 메모리에 들어가지 않는 방법
    - memmap 클래스 사용
    - MiniBatchKMeans 클래스의 partial_fit()을 사용
- 최적의 클러스터 개수 찾기
    - 이너셔는 일반적으로 k가 증가함에 따라 점점 작아지므로 k를 선택할 때 좋은 성능 지표가 아님
    - 이너셔는 k의 함수로 그래프를 그리면 엘보처럼 굴곡이 있는 지점이 생김 → 이너셔가 빠르게 감소하다 느리게 감소하는 구간
    - 실루엣 점수 사용하기
        - 실루엣 점수: 모든 샘플에 대한 실루엣 계수의 평균
        - 실루엣 계수
            
            $$
            (b-a)/max(b,a)
            $$
            
            a: 동일한 클러스터에 있는 다른 샘플까지 평균 거리
            
            b: 가장 가까운 클러스터까지 평균 거리
            
            -1부터 +1 사이의 값
            
        - 실루엣 다이어그램: 모든 샘플의 실루엣 계수를 할당된 클러스터와 계수 값으로 정렬해 그린 그래프
            
            각 클러스터마다 칼 모양의 그래프를 그림
            
            - 높이: 클러스터가 포함하고 있는 샘플의 개수
            - 너비: 클러스터에 포함된 샘플의 정렬된 실루엣 계수
            - 수직 파선: 각 클러스터 개수에 해당하는 평균 실루엣 점수

### k-평균의 한계

- 장점
    - 속도가 빠르고 확장이 용이
- 단점
    - 최적에 가까운 솔루션을 찾기 위해 여러 번의 반복이 필요
    - 클러스터 개수 지정
    - 크기 또는 밀집도가 서로 다르거나 원형이 아닐 경우 잘 작동하지 않음
        
        → 일반적으로 특성의 스케일을 맞출경우 알고리즘에 도움을 줌
        

### 군집을 사용한 이미지 분할

이미지를 여러 개의 세그먼트로 분할하는 작업

- 색상 분할: 동일한 색상을 가진 픽셀을 같은 세그먼트에 할당
- 시멘틱 분학: 동일한 종류의 물체에 속한 모든 픽셀을 같은 세그먼트에 할당
- 인스턴스 분할: 개별 객체에 속한 모든 픽셀을 같은 세그먼트에 할당

### 군집을 사용한 준지도 학습

레이블이 없는 데이터가 많고 레이블이 있는 데이터가 적을 경우 사용

- 레이블이 있는 데이터에 대해 기준 성능을 얻기
- kmeans 알고리즘 훈련시키고 대표 이미지(센트로이드와 가장 가까운 이미지) 찾기
- 대표 샘플에 수동으로 레이블 할당
- 레이블 전파: 할당한 레이블을 동일한 클러스터에 있는 모든 샘플로 전파
    - sklearn.semi_supervised의 LabelSpreading과 LabelPropagation
- 클러스터 중심에서 가장 먼 1%의 샘플을 무시 → 이상치 제거
    - SelfTrainingClassifier에 기본 분류기를 제공하면 위 과정을 반복
        
        레이블이 지정된 샘플에서 훈련한 다음 이를 사용하여 레이블이 지정되지 않은 샘플의 레이블을 예측 → 가장 확신하는 레이블로 훈련 세트를 업데이트 → 더 이상 레이블을 추가할 수 없을 때까지 반복
        

### 능동학습

전문가가 학습 알고리즘과 상호 작용하여 알고리즘이 요청할 때 특정 샘플의 레이블을 제공

- 불확실성 샘플링을 가장 많이 사용
    1. 지금까지 수집한 레이블된 샘플에서 모델을 훈련
        
        이 모델을 사용해 레이블되지 않은 모든 샘플에 대한 예측을 생성
        
    2. 모델이 가장 불확실하게 예측한 샘플(추정 확률이 낮은 샘플)을 전문가에게 보내 레이블을 붙임
    3. 레이블을 부여하는 노력만큼의 성능이 향상되지 않을 때까지 반복
- 모델을 가장 크게 바꾸는 샘플이나 모델의 검증 점수를 가장 크게 떨어뜨리는 샘플, 여러 개의 모델이 동일한 예측을 내지 않는 샘플에 대한 레이블 요청

### DBSCAN(Density-based Spatial Clustering of Applications with Noise)

밀집된 연속적 지역을 클러스터로 정의

- 알고리즘 작동 방식
    - 알고리즘이 각 샘플에서 작은 거리인 $\epsilon$내에 샘플이 몇 개 놓여있는 지 셈 ($\epsilon$-이웃)
    - $\epsilon$-이웃 내에 적어도 min_samples개 샘플이 있다면 이를 핵심 샘플로 간주
    - 핵심 샘플이 이웃에 있는 모든 샘플은 동일한 클러스터에 속함
        
        이웃에 다른 핵심 샘플이 포함될 수 있으므로 핵심 샘플의 이웃의 이웃은 계속해서 하나의 클러스터 형성
        
    - 핵심 샘플이 아니고 이웃도 아닌 샘플은 이상치로 판단
- 모든 클러스터가 밀집되지 않은 지역과 잘 구분될 때 좋은 성능을 냄
    - 사이킷런의 DBSCAN 클래스
- 장점
    - 하이퍼파라미터가 2개로 매우 간단
    - 클러스터의 모양과 개수에 상관없이 감지
    - 이상치에 안정적
- 단점
    - 클러스터 간의 밀집도가 크게 다르거나 (계층적 DBSCAN 사용 가능)
    - 일부 클러스터 주변에 저밀도 영역이 충분히 없는 경우
    
    알고리즘이 잘 작동하지 않음
    
    - 계산 복잡도가 $O(m^2n)$이므로 대규모 데이터셋에 부적절

### 다른 군집 알고리즘

- 병합 군집
    
    클러스터 계층을 밑바닥부터 위로 쌓아 구성
    
    - 반복마다 인접한 클러스터 쌍을 연결
    - 병합된 클러스터 쌍을 트리로 그려 이진트리 형성 (트리의 리프는 개별 샘플)
    - 장점
        - 다양한 형태의 클러스터 감지 가능
        - 특정 클러스터 개수를 선택하는 데 도움이 되는 클러스터 트리 생성 가능
        - 이웃한 샘플 간의 거리를 담은 희소 행렬을 연결 행렬로 전달하는 방식을 통해 대규모 샘플에 적용 가능
- BIRCH(Balanced Iterative Reducing and Clustering using Hierarchies)
    
    대규모 데이터셋을 위한 알고리즘
    
    - 특성 개수가 너무 많지 않다면(20개 이하) 배치 k-평균보다 빠르고 비슷한 결과 도축
    - 새로운 샘플을 클러스터에 빠르게 할당할 수 있는 정보를 담은 트리 구조를 생성
    - 모든 샘플을 저장하지 않음
- 평균-이동
    - 작동 방법
        - 각 샘플을 중심으로 하는 원을 그림
        - 원마다 안에 포함된 모든 샘플의 평균을 구함
        - 원의 중심을 편규점으로 이동
        - 원이 움직이지 않을 때까지 위 과정을 반복
    - 지역의 최대 밀도를 찾을 때까지 높은 쪽으로 원을 이동
    - 동일한 지역에 안착한 원에 있는 모든 샘플은 동일한 클러스터
    - 장점
        - 모양이나 개수에 상관없이 클러스터를 찾음
        - 하이퍼파라미터 개수가 적음(대역폭 설정 딱 한개)
        - 국부적인 밀집도 추정에 의존
    - 단점
        - 클러스터 내부 밀집도가 불균형할 때 클러스터를 여러 개로 나누는 경향이 있음
        - 시간복잡도가 $O(m^2)$로 대규모 데이터셋에 적합하지 않음
- 유사도 전파
    
    모든 샘플은 자신을 대표할 다른 샘플을 선택할 때까지 샘플간의 메시지를 반복적으로 교환하는 방법
    
    - 선출된 샘플을 예시라고 함
    - 각 예시와 이를 선출한 모든 샘플은 하나의 클러스터 형성
    - 선호도 전파도 비슷하게 작동
    - 장점
        - 클러스터의 중심 근처에 위치한 예시를 선택하는 경향이 있음
        - 미리 클러스터의 수를 정할 필요 없이 훈련 중에 결정
        - 다양한 크기의 클러스터 처리 가능
    - 단점
        - 시간복잡도가 $O(m^2)$로 대규모 데이터셋에 적합하지 않음
- 스펙트럼 군집
    
    샘플 사이의 유사도 행렬을 받아 저차원 임베딩 형성
    
    만들어진 저차원 공간에서 다른 군집 알고리즘을 사용
    
    - 장점
        - 복잡한 클러스터 구조를 감지하고 그래프 컷을 찾는 데 사용 가능
    - 단점
        - 샘플 개수가 많으면 잘 적용되지 않음
        - 클러스터의 크기가 매우 다를 경우 잘 작동하지 않음

## 2. 가우스 혼합(GMM: Gaussian Mixture Model)

샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 분포에서 생성되었다고 가정하는 확률 모델

- 하나의 가우스 분포에서 생성된 모든 샘플은 하나의 클러스터 형성
    
    일반적으로 타원형
    
    - 타원의 모양, 크기, 밀집도, 방향이 모두 다름
- GMM의 변형 GaussianMixture
    
    가장 간단한 버전
    
    - 사전에 가우스 분포의 개수 k를 알아야 함
    - 데이터셋 X가 아래 확률 과정을 통해 생성되었다고 가정
        - 샘플마다 k개의 클러스터에서 랜덤하게 한 클러스터가 선택
            
            j번째 클러스터를 선택할 확률은 클러스터의 가중치 $\phi^{(j)}$
            
            i번째 샘플을 위해 선택한 클러스터 인덱스 $z^{(i)}$
            
        - i번째 샘플이 j번째 클러스터에 할당되었다면($z^{(i)}=j$)
            
            이 샘플의 위치 $x^{(i)}$는 평균이 $\mu^{(j)}$이고 공분산 행렬이 $\sum^{(j)}$인 가우스 분포에서 랜덤하게 샘플링
            
            $x^{(i)}$ ~ $N(\mu^{(j)}, \sum^{(j)})$
            
    - 데이터가 X가 주어지면 가중치 $\phi$, 전체 분포의 파라미터 $\mu^{(i)}$에서 $\mu^{(k)}$까지, $\sum^{(i)}$에서 $\sum^{(k)}$까지 추정