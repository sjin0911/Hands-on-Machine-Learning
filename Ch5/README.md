# 서포트 벡터 머신

다목적 머신러닝 모델

- 매우 강력해 선형이나 비선형 분류, 회귀, 특이치 탐지에 사용 가능
- 중소규모의 비선형 데이터셋, 특히 분류 작업에 특화
- 매우 큰 데이터셋으로는 잘 확장되지 않음

## 1. 선형 SVM 분류

- SVM 분류기를 클래스 사이에 가장 폭이 넓은 도로를 찾는 것 → 라지 마진 분류(large margin classification)
- 도로 경계에 위치한 샘플에 의해 전적으로 결정 → 이러한 샘플을 서포트 벡터라고 부름
- 특성의 스케일에 민감

### 소프트 마진 분류

- 하드 마진 분류(hard margin classificatioin): 모든 샘플이 도로 바깥쪽에 올바르게 분류되어 있음
    - 데이터가 선형적으로 구분될 수 있어야 제대로 작동하며 이상치에 민감
        
        → 도로의 폭을 가능한 한 넓게 유지하는 것과 마진 오류(margin violation, 샘플이 도로 중간이나 심지어 반대쪽에 있는 경우)사이의 적절한 균형 필요
        
        ⇒ 소프트 마진 분류 
        
- 사이킷런이 SVM 모델을 만들 때 규제 하이퍼파라미터 C를 포함해 하이퍼파라미터 지정 가능
    - 낮게 설정하면 도로가 더 커지고 더 많은 마진 오류 발색
        
        → C를 줄이면 도로를 지지하는 샘플이 많아져 과대적합의 위험이 줄어듦
        
- 로지스틱 회귀와 달리 클래스 확률을 추정하는 predict_proba() 메서드가 없으므로

## 2. 비선형 SVM 분류

다항 특성과 같은 특성을 추가해 비선형 데이터셋을 다루기

- 선형적으로 구분될 수 없는 데이터셋을 특성을 추가해 선형적으로 구분되도록 만들 수 있음
- PolynomialFeatures 변환기와 StandardScaler, LinearSVC를 연결한 파이프라인을 사용

### 다항식 커널

- SVM을 사용할 떈 커널 트릭을 사용
    
    실제로는 특성을 추가하지 않으면서 매우 높은 차수의 다항 특성을 많이 추가한 것과 같은 결과를 얻게 해주는 방법
    
    - 실제론 어떤 특성도 추가하지 않음

### 유사도 특성

각 샘플이 특정 랜드마크와 얼마나 닮았는지 측정하는 유사도 함수로 계산한 특성을 추가하는 방법 

- 랜드마크를 선택하는 방법은 데이터셋에 있는 모든 샘플 위치에 랜드마크를 설정
    
    → 차원이 매우 커져 변환된 훈련 세트가 선형적으로 구분될 가능성이 커짐
    

### 가우스 RBF 커널

커널 트릭을 사용해 유사도 특성을 많이 추가하는 것과 비슷한 결과를 얻음

- 하이퍼파라미터 $\gamma$가 규제 역할을 함

> 다른 커널도 있지만 거의 사용되지 않고 문자열 커널이 가끔 사용됨
> 
> 
> 언제나 선형 커널을 가장 먼저 시도하고 훈련 세트가 너무 크지 않다면 가우스 RBF 커널도 시도해보기
> 

### 계산 복잡도

- LinearSVC 파이썬 클래스가 기반으로 하는 Liblinear 라이브러리
    
    선형 SVM을 위한 최적화된 알고리즘을 구현
    
    - 커널 트릭을 지원하지 않지만 훈련 샘플과 특성 수에 거의 선형적으로 증가
    
    → $O(m*n)$
    
    - 정밀도를 높이면 알고리즘의 수행 시간이 길어짐 → 오차 허용 파라미터 $\epsilon$으로 조정 (사이킷런의 매개변수 tol)
        
        대부분의 분류 문제는 기본값으로도 잘 작동
        
- SVC가 기반으로 하는 libsvm 라이브러리
    
    커널 트릭 알고리즘을 구현
    
    - 훈련 샘플 수가 커지면 매우 느려짐
        
        $O(m^2*n)$과 $O(m^3*n)$ 사이
        
    - 특성 수에 대해서, 특히 희소 특성에 대해서는 잘 확장
        
        0이 아닌 특성의 평균 수에 비례
        
- SGDClassifier 클래스는 라지 마진 분류를 수행하고 규제 파라미터를 조정해 선형 SVM과 유사한 결과를 생성
    - 확률적 경사 하강법(점진적 학습, 적은 메모리 사용)을 사용해 RAM에 맞지 않는 대규모 데이터셋을 훈련가능
        
        → $O(m*n)$
        

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/6a620654-f6e8-4390-85bf-a1a0e09a03d0/Untitled.png)

## 3. SVM 회귀

SVM 회귀는 제한된 마진 오류 안에서 도로 안에 가능한 한 많은 샘플이 들어가도록 학습

- 도로의 폭은 하이퍼파라미터 $\epsilon$으로 조절
    - 마진 안에서는 훈련 샘플이 추가 되어도 모델의 예측에 영향이 없음
    - $\epsilon$을 줄이면 서포트 벡터의 수가 늘어나서 모델이 규제 → $\epsilon$에 민감하지 않다
- 비선형 회귀 작업을 처리할 땐 커널 SVM 모델을 사용
    
    SVR은 SVC의 회귀 버전
    
    - LinearSVR은 필요한 시간이 훈련 세트의 크기에 비례해서 선형적으로 증가
        
        SVR은 훈련 세트가 커지면 매우 느려짐
        

## 4. SVM 이론

마진 오류 횟수를 제한하면서 도로(또는 마진)를 가능한 한 넓게 만드는 가중치 벡터 w와 편향 b를 찾아야 함

- 도로의 너비
    - 도로의 너비를 더 넓히려면 w를 더 작게 만들어야 함
        
        편향 b는 마진의 크기에 영향을 미치지 않고 위치만 이동시킴
        
- 마진오류를 피하기
    - 따라서 결정 함수가 모든 양성 훈련 샘플에서는 1보다 커야 하고 음성 훈련 샘플에서는 -1보다 작아야 함
        
        앞서 말한 제약 조선을 모든 샘플에서 $t^{(i)}(w^Tx^{(i)}+b)≥1$로 표현 가능
        
    - 하드 마진 선형 SVM 분류기의 목적 함수를 제약이 있는 최적화 문제로 표현 가능
        
        $$
        minimize_{w,b} {1 \over 2}w^Tw
        $$
        
- 소프트 마진 분류기의 목적 함수를 구성하기 위해 각 샘플에 대해 슬랙 변수를 도입
    - $\zeta^{(i)}>=0$을 도입
        
        i번째 샘플이 얼마나 마진을 위반할 지 결정
        
    - 마진 오류를 최소화하기 위해 가능한 한 슬랙 변수의 값을 작게 만드는 것과 마진을 크게 하기 위해 $w^Tw*(1/2)$를 가능한 한 작게 만드는 것의 상충
        
        하이퍼파라미터가 두 목표 사이의 트레이드오프를 결정
        
        $$
        minimize_{\omega,b,\zeta} {1 \over 2}w^Tw+C\sum_{i=1}^m\zeta^{(i)}
        $$
        
        [조건] i = 1, 2, …, m일 때 $t^{(i)}(w^Tx^{(i)}+b)≥1-\zeta^{(i)}$이고 $\zeta^{(i)}≥0$
        
- 콰드라틱 프로그래밍(QP) 문제 : 하드 마진과 소프트 마진 문제는 모두 선형적인 제약 조건이 있는 볼록 함수의 이차 최적화 문제
- SVM을 훈련하는 방법
    - QP 솔버를 사용
    - 경사 하강법을 사용하여 힌지 손실 또는 제곱 힌지 손실을 최소화
        - 양성 클래스 샘플 x가 주어졌을 때 결정 함수의 출력 s가 1보다 클 경우 손실은 0
            
            → 도로에서 벗어나 양성 클래스 쪽에 있는 경우
            
        - 음성 클래스 샘플이 주어졌을 때 결정 함수의 출력 s가 -1보다 작거나 같으면 손실이 0
            
            → 도로에서 벗어나 음성 클래스 쪽에 있는 경우
            
            ⇒ 샘플이 마진에서 반대로 멀어질수록 손실이 커짐
            
            - 제곱 힌지를 사용할 경우 이상치에 더 민감하게 반응(손실이 이차식으로 증가)하지만 더 빨리 수련하는 경향이 있음
        - LinearSVC는 제곱된 힌지 손실을 사용하는 반면 SGDClassifier는 힌지 손실을 사용 → loss 하이퍼파라미터를 설정해 손실 선택 가능

## 5. 쌍대 문제

SVM 문제는 원 문제 또는 쌍대 문제 중 어느것을 선택해도 같은 해를 제공

???

### 커널 SVM