# 앙상블 학습과 랜덤 포레스트

- 앙상블 학습: 일련의 예측기(앙상블)로부터 예측을 수집
    
    → 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있음 
    
    - 예시
        
        훈련 세트로부터 랜덤으로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련
        
        예측을 모아 가장 많은 선택을 받은 클래스를 앙상블의 예측으로 선택
        
- 앙상블 방법: 앙상블 학습 알고리즘
- 랜덤 포레스트: 결정 트리의 앙상블 → 간단하지만 현재 가장 강력한 머신러닝 알고리즘
    
    프로젝트의 마지막에선 흔히 앙상블 방법을 사용해 여러 괜찮은 예측기를 연결해 더 좋은 예측기를 만듦
    

## 1. 투표 기반 분류기

- 직접 투표(hard votin) 분류기: 다수결 투표로 정해지는 분류기
    
    앙상블에 포함된 개별 분류기 중 가장 뛰어난 것보다 정확도가 높은 경우가 많음
    
    - 약한 학습기: 랜덤 추측보다 조금 더 높은 성능을 내는 분류기
    - 강한 학습기: 높은 정확도
    
    → 큰 수의 법칙(반복이 많아질수록 비율이 점점 더 정확해짐)때문에  약한 학습기의 앙상블로 강한 학습기를 만들 수 있음
    
- 정확도는 모든 분류기가 완벽하게 독립적이고 오차에 상관관계가 없어야 가능
    
    → 이같은 경우 같은 데이터로 훈련되기 때문에 불가능
    
    분류기들은 같은 종류의 오차를 만들기 쉽고 잘못된 클래스가 다수인 경우가 많고 앙상블의 정확도가 낮아짐
    
    - 각기 다른 알고리즘으로 학습시키면 다른 종류의 오차를 만들어 앙상블 모델의 정확도가 향상
- sklearn의 VotingClassifier
    - 이 클래스는 모든 추정기를 복제하여 복제된 추정기를 혼련
    - predict() 메서드 호출을 통해 직접 투표 수행 가능
- 간접 투표: 모든 분류기가 확률을 예측할 수 있을 경우, 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측
    
    → 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식보다 성능이 높음
    
    - 투표 기반 분류기의 “voting=soft”로 설정하고 모든 분류기가 클래스의 확률을 추정하도록 하기

## 2. 배깅과 페이스팅

다양한 분류기를 만드는 방법 중 하나는 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습시키는 것

- 서브셋을 구성하는 방법
    - 배깅(Bootstrap Aggregating): 훈련 세트에서 중복을 허용하여 샘플링하는 방식
    - 페이스팅: 중복을 허용하지 않고 샘플링하는 방식
    
    두 방법모두 같은 훈련 샘플을 여러 개의 예측기에 사용 가능하지만 한 예측기를 위해 같은 훈련 샘플을 여러번 샘플링 하는 것은 배깅만 가능
    
- 집계함수
    - 통계적 최빈값: 일반적으로 분류일 때 사용, 직접 투표 분류기처럼 가장 많은 예측 결과
    - 회귀일 경우 평균을 사용
    
    개별 예측기는 원본 훈련 세트로 훈련시킨 것보다 훨씬 크게 편향되어 있지만 집계함수를 통과하면 편향과 분산이 모두 감소
    
    앙상블의 결과는 하나의 예측기를 훈련시킬 때와 편향은 비슷하지만 분산은 감소
    
- 예측기는 다른 CPU 코어나 서버에서 병렬로 학습 가능 → 확장성이 좋음

### 사이킷런의 배깅과 페이스팅

- 앙상블은 비슷한 편향에서 더 작은 분산을 생성
    
    훈련 세트의 오차 수가 거의 동일하지만 결정 경계는 덜 불규칙
    
- BaggingClassifier(회귀 → BaggingRegressor)
    - n_jobs 매개변수: 훈련과 예측에 사용할 CPU 코어 수 지정 (-1: 가용한 모든 코어)
    - 클래스 확률이 추정 가능하면 직접 투표 대신 자동으로 간접 투표 방식 사용
    - 각 예측기가 학습하는 서브셋에 다양성을 추가하므로 페이스팅보다 편향이 조금 더 높음
        
        하지만 예측들간의 상관관계를 줄이므로 앙상블의 분산이 감소
        
        → 전반적으로 배깅이 더 선호됨
        

### OOB 평가

- 배깅의 샘플 선택 방식
    
    평균적으로 각 예측기에 훈련 샘플의 63% 정도만 샘플링
    
    - OOB(out-of-bag) 샘플: 선택되지 않은 37%의 샘플
        
        이 샘플들은 훈련 과정에 참여하지 않으므로 예측기 평가에 사용 가능
        
    - 앙상블의 평가: 각 OOB 평가를 평균
        
        BaggingClassifier의 “oob_score=True”를 통해 OOB 평가 수행 가능
        

## 3. 랜덤 패치와 랜덤 서브스페이스

- BaggingClassifier가 지원하는 특성 샘플링
- max_features, bootstrap_features 두 매개변수로 조절
    
    max_samples, bootstrap과 동일하지만 특성에 대한 샘플링에 사용
    
    → 각 예측기는 랜덤으로 선택한 입력 특성의 일부분으로 훈련
    
- 훈련 속도를 크게 높일 수 있으므로 매우 고차원의 데이터 셋에 유리
- 랜덤 패치 방식: 훈련 특성과 샘플을 모두 샘플링
- 랜덤 서브스페이스 방식: 훈련 샘플을 모두 사용하고 특성을 샘플링하는 것
- 특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리고 분산을 낮춤

## 4. 랜덤 포레스트

일반적으로 배깅 배깅 방법(또는 페이스팅)을 적용한 결정 트리의 앙상블

- max_samples를 훈련 세트의 크기로 지정
- RandomForestClassifier(회귀 → RandomForestRegressor)
    
    DecisionTreeClassifier와 BaggingClassifier의 매개변수를 모두 가지고 있음
    
- 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 랜덤으로 선택한 특성 후보 중에서 최적의 특성을 찾음 → 무작위성 주입
    - 기본적으로 $\sqrt{n}$개의 특성(n: 특성 개수)을 선택
        
        → 트리의 다양성 확보와 편향을 대가로 분산을 낮춤
        

### 엑스트라 트리

랜덤 포레스트가 트리를 만들 때 각 노드는 랜덤으로 특성의 서브셋을 만들어 분할에 사용

- 익스트림 랜덤 트리(엑스트라 트리): 극단적으로 랜덤
    
    트리를 더욱 랜덤하게 만들기 위해 최적의 임곗값을 찾는 대신 각 특성에 대해 핸덤한 임곗점을 사용
    
    DecisionTreeClassifier의 “splitter=”random””을 사용
    
- 엑스트라 트리의 훈련 속도가 비교적 빠름
    
    가장 최적의 임곗값을 찾는 것은 트리 알고리즘에서 시간이 가장 많이 소요되는 작업
    
- ExtraTreesClassifier를 사용:
    - RandomForestClassifier의 boostrap 매개변수가 False인 버전

### 특성 중요도

특성의 상대적 중요도 측정이 쉬움

- 특성의 중요도: 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 → 가중치 평균 (가중치 = 연관된 훈련 샘플 수)
- 사이킷런이 자동으로 특성 중요도 점수를 계산하고 합이 1이 되도록 정규화
- 특성을 선택해야 할 경우 어떤 특성이 중요한지 빠르게 확인 가능

## 5. 부스팅

약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법

- 앞의 모델을 보완해 나가며 일련의 예측기를 학습

### AdaBoost(Adaptive Boosting)

이전 모델이 과소적합했던 훈련 샘플의 가중치를 높여 이전 예측기를 보완하는 방법

학습하기 어려운 샘플에 점점 더 맞춰짐 

- AdaBoost 분류기를 만드는 방법
    1. 알고리즘의 기반이 되는 첫 번째 분류기를 훈련 세트에서 훈련시키고 예측을 만듦
    2. 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높임
    3. 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 예측을 만듦
    4. 가중치 업데이트
    
    → 위 3, 4번 과정을 반복
    
- 성능이 좋아지도록 앙상블에 예측기를 추가
- 모든 예측기가 훈련을 마치면 이 앙상블은 배깅이나 페이스팅과 비슷한 방식으로 예측을 만듦
- 연속된 학습 기법의 단점
    
    각 예측기는 이전 예측기가 훈련되고 평가된 후에 학습을 진행해야함
    
    → 훈련의 병렬화 불가능 (배깅이나 페이스팅만큼 확장성이 높지 않음)
    
- AdaBoost 알고리즘
    
    각 
    
    - j번째 예측기의 가중치가 적용된 오류율($r_j$)
        
        각 샘플 가중치($w^{(i)}$)는 1/m으로 초기화
        
        $$
        r_j={{\sum_{i=1}^m w^{(i)}}_{\hat h_j^{(i)}\neq y^{(i)}} \over {\sum_{i=1}^m w^{(i)}}}
        $$
        
    - 예측기 가중치 ($\alpha_j$)
        
        학습률 파라미터($\eta$): 기본값은 1
        
        $$
        \alpha_j=\eta \log{{1-r_j} \over r_j}
        $$
        
        - 예측기가 정확할수록 가중치는 높아짐
        - 랜덤추측이라면 0에 가깝고 랜덤 추측보다 정확도가 낮을 경우 음수
    - 가중치 업데이트 규칙
        
        $$
        w^{(i)} \leftarrow \begin{cases}
        w^{(i)} & \hat y_j^{(i)}=y^{(i)}일 때 \\
        w^{(i)} exp(\alpha_j) & \hat y_j^{(i)}\neq y^{(i)}일 때 \end{cases}
        $$
        
    - 이 후 모든 샘플의 가중치를 $\sum_{i=1}^m w^{(i)}$로 나눔
    - 새 예측기가 업데이트된 가중치를 사용해 훈련되고 전체 과정 반복
- 예측을 할 때 모든 예측기의 예측을 계산하고 예측기 가중치 $\alpha_j$를 더해 예측 결과를 만들고 가중치 합이 가장 큰 클래스가 예측 결과가 됨

### 그레이디언트 부스팅

앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가

- AdaBoost처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여오차에 새로운 예측기를 학습시킴
- 그레이디언트 트리 부스팅, 그레이디언트 부스티드 회귀 트리: 결정 트리를 기반 예측기로 사용하는 회귀 문제
- 축소(규제의 방법)
    
    각 트리의 기여도를 낮게 설정하면 앙상블을 훈련 세트에 학습시키기위해 많은 트리가 필요하지만 전반적으로 예측의 성능은 좋아짐
    
    - 트리가 매우 많아지면 훈련 세트에 과대적합
- 최적의 트리 개수를 찾는 방법
    
    n_iter_no_change 하이퍼파라미터를 정숫값으로 설정
    
    - 마지막 10개의 트리가 도움이 되지 않는 경우 GBR이 자동으로 트리 추가를 중지
    - fit( ) 메서드가 자동으로 훈련 세트를 더 작은 훈련 세트와 검증 세트로 분할하기 때문에 트리가 추가될 때마다 모델 성능 평가 가능
    - validation_fraction: 검증 세트의 크기, default 10%
    - tol: 무시할 수 있는 최대 성능 향상, default 0.0001
    - subsample: 각 트리가 훈련할 때 사용할 훈련 샘플의 비율
        - 편향이 높아지는대신 분산이 낮아지고 훈련 속도가 빨라짐 
        → 확률적 그레이디언트 부스팅

### 히스토그램 기반 그레이디언트 부스팅

입력 특성을 구간으로 나누어 정수로 대체

- max_bins를 통해 구간의 개수를 제어 (default 255)
- 장점
    - 알고리즘이 평가해야 하는 가능한 임곗값의 수를 줄임
    - 정수로 작업해 더 빠르고 메모리 효율적인 데이터 구조 사용
    - 각 트리를 학습할 때 특성을 정렬할 필요가 없음
        
        → $O(b*m)$ (b: 구간의 개수, m: 훈련 샘플 개수, n: 특성 개수)
        
        대규모 데이터셋에서 GBRT보다 수백 배 빠르게 훈련 가능
        
- 단점
    - 정밀도 손실 유발 → 과소적합 유발 가능
- HistGradientBoostingRegressor
    - 범주형(0~max_bins) 특성과 누락된 값을 지원 → 전처리 과정 간소화

## 6. 스태킹

앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련

- 블렌더, 메타 학습기: 마지막 최종 예측기
- 블렌더를 훈련하기 위해선 블렌딩 훈련 세트가 필요
    - 블렌더를 훈련하기 위한 입력 특성
        
        앙상블의 모든 예측기에서 cross_val_predict()를 사용해 원본 훈련 세트에 있는 각 샘플에 대한 표본 외 예측을 얻음
        
    - 타깃
        
        원본 훈련 세트에서 간단히 복사
        
    - 원본 훈련 세트의 특성 개수에 관계없이 블렌딩 훈련 세트에는 예측기당 하나의 입력 특성이 포함
    - 블렌더가 학습되면 기본 예측기는 전체 원본 훈련 세트로 마지막에 한 번 더 재훈련
- 여러 가지 블렌더를 훈련한 뒤 전체 블렌더 계층을 얻은 다음 그 위에 다른 블렌더를 추가해 최종 예측 생성 가능
    
    → 복잡성 측면에서 비용이 증가
    
- 사용 가능한 경우 각 예측기에 대해 predict_proba()를 호출하고, 불가능할 경우 decision_function()을 사용하거나 마지막 수단으로 predict()를 사용
- 최종 예측기 default
    - StackingClassifier → LogisticRegression
    - StackingRegressor → RidgeCV

## 연습문제

1. 앙상블 방법을 사용해 성능을 향상시키기 → 대부분의 예측기는 앙상블 방법을 사용할 경우 성능이 올라감
    
    모델이 서로 다를수록, 서로 다른 훈련 샘플에서 훈련되었을 경우 훨씬 성능이 좋아짐
    
2. 직접 투표는 예측이 다수결 투표로 정해지고, 간접 투표는 개별 분류기의 예측 확률 추정값을 평균 내어 사용하는 방법
    
    간접 투표 방식이 신뢰가 높은 투표에 더 큰 가중치를 주기 때문에 종종 더 나은 성능을 냄
    
3. 배깅, 페이스팅과 랜덤 포레스트의 경우 각 예측기가 독립적이므로 여러 대의 서버에 분산하여 훈련 속도를 높일 수 있음
    
    부스팅 앙상블의 경우 훈련이 순차적으로 이루어져야 하므로 여러 대의 서버에 분산해도 같은 시간이 소요됨
    
    스태킹 앙상블의 경우 한 층의 예측기는 독립적이므로 여러 대의 서버에서 병렬로 훈련될 수 있음. 하지만 다른 층의 예측기는 이전 층의 예측기가 훈련된 후 훈련될 수 있으므로 병렬화 불가능
    
4. OOB를 사용하면 배깅 앙상블의 각 예측기가 훈련에 포함되지 않은 샘플을 사용해 평가되고 추가적인 검증 세트가 없어도 편향되지 않게 앙상블을 평가하도록 도와줌 
    
    훈련에 더 많은 샘플을 사용할 수 있으므로 앙상블의 성능이 향상 
    
5. 추가적인 무작위성은 예측의 편차를 증가시키지만 분산을 감소시킴, 훈련 데이터에 과대적합될 위험을 감소시킴
    
    가장 최적의 임곗값을 찾는 과정이 랜덤 포레스트에서 가장 오래걸리는 작업이므로 엑스트라 트리 분류기는 일반 랜덤 포레스트보다 빠름
    
    예측 시간의 경우 일반 랜덤 포레스트와 엑스트라 트리가 비슷함
    
6. 최대 트리의 개수를 증가시키거나 기반 예측기의 규제 하이퍼파라미터를 감소하거나 학습률을 조금 증가시킬 수 있음
7. 학습률을 낮춰 너무 많은 트리의 생성을 막아야 함
    
    알맞은 개수를 찾기 위해 조기 종료 사용