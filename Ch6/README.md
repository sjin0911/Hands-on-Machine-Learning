# 결정 트리

분류와 회귀 작업, 다중 출력 작업까지 가능한 다목적 머신러닝 알고리즘

## 1. 결정 트리 학습과 시각화

- graphviz를 사용
    
    오픈 소스 그래프 시각화 소프트웨어 패키지
    
    - .dot 파일을 pdf 또는 png와 같은 다양한 형식으로 변환하는 dot 명령줄 도구가 포함

## 2. 예측

- 결정 트리의 장점 중 하나는 데이터 전처리 과정이 거의 필요하지 않음
    
    → 특성의 스케일을 맞추거나 원점에 맞추는 작업이 필요하지 않음
    
- 루트 노드(Root node, 깊이가 0인 맨 꼭대기의 노드)에서 시작
- 리프 노드(Leaf node, 자식 노드를 가지지 않는 노드)에 도달할 때까지 데이터를 검사해 분할 노드를 따라 내려감
- 각 노드의 속성
    - sample: 얼마나 많은 훈련 샘플이 적용되었는지 헤아린 것
    - value: 노드에서 각 클래스에 얼마나 많은 훈련 샘플이 있는지 알려줌
    - gini: 지니 불순도(gini impuritty)를 측정
        
        한 노드의 모든 샘플이 같은 클래스에 속해 있다면 이 노드를 순수(gini=0)하다고 함
        
        $$
        G_i=1-\sum_{k=1}^nP_{i,k}^2
        $$
        
        $P_{i,k}$: i번째 노드에 있는 훈련 샘플 중 클래스 k에 속한 샘플의 비율
        

## 모델 해석

- 화이트박스: 직관적이고 결정 방식을 이해 가능 → 결정 트리
- 블랙박스: 성능이 뛰어나고 예측을 만드는 연산 과정을 쉽게 확인 할 수 있지만 왜 그런 예측을 만드는지 쉽게 설명하기 어려움 → 랜덤 포레스트, 신경망

## 3. 클래스 확률 추정

결정 트리는 한 샘플이 특정 클래스 k에 속할 확률 추정 가능 

- 이 샘플의 리프 노드를 찾기 위해 트리를 탐색하고 그 노드에 있는 클래스 k의 훈련 샘플의 비율을 반환
- classifier의 predict_proba를 이용해 추정 확률 출력 가능
- predict를 통해 추정 결과 출력
- 추정 확률은 같은 리프 노드에 위치할 경우 같음

## 4. CART 훈련 알고리즘

사이킷런은 결정 트리를 훈련시키기 위해 CART(Classification And Regression Tree) 알고리즘을 사용

- 훈련 세트를 하나의 특성 k의 임곗값 $t_k$를 사용해 두 개의 서브셋으로 나눔
    
    → 크기에 따른 가중치가 적용된 가장 순수한 서브셋으로 나눌 수 있는 $(k, t_k)$ 쌍을 찾음 
    
- 분류에 대한 CART 비용 함수
    
    $$
    J(k, t_k)= {m_{left} \over m}G_{left}+{m_{right} \over m}G_{right}
    $$
    
    $G_{left/right}$: 왼쪽/ 오른쪽 서브셋의 불순도
    
    $m_{left/right}$: 왼쪽/ 오른쪽 서브셋의 샘플 수
    
- CART 알고리즘은 훈련 세트를 둘로 나누고 같은 방식으로 서브셋을 또 나누는 작업을 반복
    
    최대 깊이(max_depth)가 되거나 불순도를 줄이는 분할을 찾을 수 없을 때 멈춤
    
- CART 알고리즘은 탐욕 알고리즘
    
    맨 위 루트 노드에서 최적의 분할을 찾으며 이어지는 각 단계에서 이 과정을 반복
    
    이 분할 과정이 가장 낮은 불순도로 이어질지는 미지수 
    
    - 최적의 트리를 찾는 문제는 NP-Complete 문제로 적당한 솔루션에서 중지해야 함

## 5. 계산 복잡도

예측 = 결정 트리를 루트 노드에서부터 리프 노드까지 탐색하는 과정

- 결정 트리는 거의 균형을 이루고 있으므로 탐색하는 데 약 $O(log_2(m))$개의 노드를 거쳐야 함
- 각 노드는 하나의 특성값만 확인하기 때문에 특성 수와 전체 복잡도는 무관
    
    → 큰 훈련 세트를 다룰 때도 예측 속도가 매우 빠름
    
- 훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든 특성을 비교
    
    → 훈련 복잡도가 $O(n*mlog_2(m))$
    

## 6. 지니 불순도 또는 엔트로피?

기본적으로 DecisionTreeClassifier 클래스는 지니 불순도를 사용

criterion 매개변수를 “entropy”로 지정할 경우 엔트로피 불순도 사용 가능

- 엔트로피: 불순도의 측정 방법
    
    어떤 세트가 한 클래스의 샘플만 담고 있다면 엔트로피가 0
    
    $$
    H_i=-{\sum_{k=1}^n}_{P_{i,k}} P_{i,k}log_2(P_{i,k})
    $$
    
- 지니 불순도와 엔트로피 모두 만들어내는 트리에 큰 차이가 없음
- 지니 불순도가 조금 더 계산이 빨라 기본값으로 적절
- 지니 불순도는 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있는 반면 엔트로피는 조금 더 균형 잡힌 트리를 만듦

## 7. 규제 매개변수

결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없음

→ 제한을 두지 않기 때문에 트리가 훈련 데이터에 아주 가깝게 맞추려고 해 과대적합되기 쉬움

- 결정 트리는 모델 파라미터가 전혀 없는 것이 아니라 훈련 되기 전에 파라미터 수가 결정되지 않기 때문에 비파라미터 모델(Nonparametric model)이라고 함
    
    → 모델 구조가 데이터에 맞춰져서 고정되지 않고 자유로움
    
    ↔ (선형 모델 등) 파라미터 모델(Parametric model)은 자유도가 제한되어 과대적합 위험이 줄고 과소적합 위험이 커짐
    
- 결정 트리의 최대 깊이: max_depth, 줄이면 모델이 규제되어 과대적합 위험 감소
- DecisionTreeClassifier의 하이퍼파라미터들
    - max_features: 각 노드에서 분할에 사용할 특성의 최대 수
    - max_leaf_nodes: 리프 노드의 최대 수
    - min_samples_split: 분할되기 위해 노드가 가져야 하는 최소 샘플 수
    - min_samples_leaf: 리프 노드가 생성되기 위해 가지고 있어야 할 최소 샘플 수
    - min_weight_fraction_leaf: min_samples_leaf와 같지만 가중치가 부여된 전체 샘플 수에서의 비율
    
    → min_으로 시작하는 매개변수를 증가, max_로 시작하는 매개변수를 감소시키면 규제가 커짐
    
- 제한 없이 결정 트리르 훈련시키고 불필요한 노드 가지치기하는 알고리즘
    - 순도를 높이는 것이 통계적으로 효과가 없다면 리프 노드 바로 위으 노드는 불필요
        
        $\chi^2$ 검정 같은 통계적 검정을 사용해 우연한 향상인지 확인
        
        이 확률을 p-값이라고 하고 임곗값(통상 5%)보다 높으면 노드 삭제
        
        → 불필요한 노드가 모두 없어질 때까지 반복
        

## 8. 회귀

결정 트리는 회귀 문제에도 사용

- 회귀 트리는 분류 트리와 매우 비슷하지만 각 노드에서 클래스를 예측하는 대신 어떤 값을 예측
    
    리프 노드의 value 값(해당 리프 노드에 있는 훈련 샘플의 평균 타깃값)을 예측으로 사용 
    
- 회귀를 위한 CART 비용 함수
    
    $$
    J(k,t_k)={ m_{len} \over m}MSE_{left}+{m_{right} \over m}MSE_{right}
    
    $$
    
    $$
    여기서 \begin{cases} MSE_{mode}={1 \over m_{node}}\sum_{i \in node}(\hat y_{nod}-y^{(i)})^2 \\
    \hat y_{node}={1 \over m_{node}}\sum_{i \in node}y^{(i)} \end{cases}
    $$
    
- 회귀 또한 결정 트리가 과대적합되기 쉬움

## 9. 축 방향에 대한 민감성

결정 트리는 계단 모양의 결정 경계를 만듦 → 데이터의 방향에 민감

- 데이터의 스케일을 조정한 다음 주성분 분석(PCA, Principle Component Analysis) 변환을 적용
    
    특성간의 상관관계를 줄이는 방식으로 데이터르 ㄹ회정
    

## 10. 결정 트리의 분산 문제

결정 트리는 분산이 큼 → 하이퍼파라미터나 데이터를 조금만 변경해도 매우 다른 모델 생성

사이킷런의 결정 트리 알고리즘은 확률적이라 동일한 데이터에서 동일한 결정 트리를 재훈련하더라도 다른 모델 생성 가능

- 여러 결정 트리의 예측을 평균
    
    이러한 결정 트리의 앙상블을 랜덤 포레스트라고 함
    

## 연습문제

1. m개의 리프 노드를 포함한 균형이 잘 잡힌 이진 트리의 깊이는 $log_2(m)$의 반올림과 같다 
    
    이진 결정 트리를 제한을 두지 않고 훈련시키면 훈련 샘플마다 하나의 리프 노드가 생성되므로 어느정도 균형이 잡힌 트리가 되고 그렇기 때문에 백만개의 샘플의 결정트리의 깊이는 20이 됨
    
2. 일반적으로 그 부모 노드보다 지니 불순도가 낮음
    
    지니 불순도의 가중치 합이 최소화되는 방향으로 각 노드를 분할하는 CART 알고리즘의 비용 함수 때문이다
    
    다른 자식 노드의 지니 불순도 감소량이 어떤 노드의 불순도 증가량보다 큰 경우 부모의 불순도보다 큰 노드가 생김 (다른 노드가 순수 노드에 가까워지는 대가)
    
3. max_depth를 줄여 규제를 강화시키는 역할을 해야 함
4. 입력 특성의 스케일은 결정트리에 영향을 주지 않음
5. 약 10배 즉, 10시간이 걸림
6. 특성 개수가 두배로 늘어나면 훈련 시간도 약 두배로 늘어남