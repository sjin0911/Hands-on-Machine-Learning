# 케라스를 이용한 인공 신경망 소개

인공 신경망(ANN: Artificial Neural Network): 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델

- 다재다능하고 확장성이 좋음
- 사용 예시
    - 수백만 개의 이미지를 분류(구글 이미지)
    - 음성 인식 서비스의 성능을 높임(애플의 시리)
    - 매일 수억 명에 이르는 사용자에게 비디오 추천(유튜브)
    
    → 대규모 머신러닝 문제를 다루는 데 적합
    

## 1. 생물학적 뉴런에서 인공 뉴런까지

- 최초의 인공 신경망 구조: 명제 논리를 사용해 동물 뇌의 생물학적 뉴런이 복잡한 계산을 위해 어떻게 상호작용하는지에 관한 간단한 계산 모델

### 생물학적 뉴런

- 생물학적 뉴런의 구성
    
    핵을 포함하는 세포체(Cell Body)와 복잡한 구성 요소로 이루어짐 
    
    - 수상돌기(Dendrite): 나뭇가지 모양의 돌기
    - 축삭돌기(Axon): 아주 긴 돌기, 세포치의 몇배에서 4만 배의 길이까지 다양
    - 축삭끝가지(Telodendria): 축삭돌기 끝의 가지
    - 시냅스 말단(Synaptic Terminals): 미세한 구조이며 다른 뉴런의 수상돌기나 세포체에 연결됨, 축삭끝가지의 끝
- 활동 전위(AP: Action Potential), 신호(Signal): 생물학적 뉴런이 만드는 짧은 전기 자극
    
    이 신호는 축삭돌기를 따라 이동해 시냅스가 신경 전달 물질(Neurotransmitter)이라는 화학적 신호를 생성
    
    일천분의 몇 초 동안 충분한 양의 신경 전달 물질을 받았을 때 자체적인 신호를 발생
    
- 생물학적 뉴런 하나는 단순하게 작동하지만 보통 수십억 개로 구성된 거대한 네트워크로 조직
    
    거대한 네트워크는 보통 다른 뉴런 네트워크 수천 개와 연결
    
    → 매우 복잡한 계산 수행 가능
    
- 생물학적 신경망(BNN: Biological Neural Network)이러한 생물학적 뉴런을 바탕으로 만든 신경망

### 뉴런을 사용한 논리 연산

인공 뉴런(Artificial Neuron): 생물학적 뉴런에서 착안한 매우 단순한 신경망 모델

- 하나 이상의 이진 입력(on/off)과 하나의 이진 출력을 가짐
- 단순히 입력이 일정 개수만큼 활성화되었을 때 출력을 내보냄
- 논리 연산을 수행하는 간단한 인공 신경망
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/d6bd9645-6d45-4665-8763-df14d0d74897/image.png)
    

### 퍼셉트론

가장 간단한 인공 신경망 구조 

- TLU(Threshold Logic Unit) 또는 이따금 LTU(Linear Threshold Unit)이라는 인공 뉴런을 기반으로 사용
    
    입력과 출력이 이진값이 아닌 어떤 숫자이고 각각의 입력 연결은 가중치와 관련되어 있음
    
    - 모델 파라미터는 입력 가중치 $\mathbf{w}$와 편향 $b$
- 작동 과정
    1. (TLU) 입력의 선형 함수 계산
        
        $z=w_1x_1+w_2x_2+\ldots+w_nx_n+b=\mathbf{w}^T\mathbf{x}+b$
        
    2. 이 결과에 계단함수(Step Function)를 적용
        
        $h_\mathbf{w}(\mathbf{x})=step(z)$
        
    
    → 로지스틱 회귀와 거의 비슷하지만 로지스틱 함수 대신 계단 함수를 적용한다는 점이 다름
    
- 헤비사이드 계단 함수(Heaviside Step Function): 가장 널리 사용되는 계단 함수(이따금 부호 함수(Sign Function)을 대신 사용)
    
    $$
    heaviside(z)=\begin{cases} 
    0 & z<0 \\
    1 & z>=0 
    \end{cases}
    
    \\
    sgn(z) = \begin{cases}
    -1 & z<0 \\
    0 & z=0 \\
    +1 & z>0 
    \end{cases}
    $$
    
- 하나의 TLU는 간단한 선형 이진 분류 문제에 사용
    
    입력의 선형 함수를 계산해 그 결과가 임곗값을 넘으면 양성 클래스를 출력(로지스틱 회귀나 선형 SVM 분류기와 비슷)
    
- TLU를 훈련 = 최적의 $\mathbf{w}$와 $b$를 찾음
- 퍼셉트론은 하나의 층 안에 놓인 하나 이상의 TLU로 구성
    - 완전 연결 층(Fully Connected Layer), 밀집 층(Dense Layer): 각각의 TLU는 모든 입력에 연결
        - 완전 연결 층의 출력 계산
            
            $$
            h_{\mathbf{w},b}(\mathbf{X})
            = \phi(\mathbf{XW}+b)
            $$
            
            $\mathbf{x}$: 입력 특성의 행렬
            
            $\mathbf{w}$: 가중치 행렬, 모든 연결 가중치를 포함, 행은 입력에 해당하고 열은 출력 층에 있는 뉴런에 해당 (랜덤으로 초기화)
            
            $b$: 뉴런마다 하나씩 모든 편향을 포함 (0으로 초기화)
            
            $\phi$: 활성화 함수(Activation Function), 인공 뉴런이 TLU일 경우 이 함수는 계단 함수 
            
    - 입력 층(Input Layer): 입력이 구성하는 층
    - 출력 층(Output Layer): 최종 출력을 생성하는 츨
- 퍼셉트론의 훈련 방식
    - 헤브의 규칙, 헤브 학습(Hebbian Learning)두 뉴런이 동시에 활성화될 때마다 이들 사이의 연결 가중치가 증가하는 경향이 있음
        
        퍼셉트론은 조금 변형된 규칙을 사용 
        
        → 오차가 감소되도록 연결을 강화 
        
    - 퍼셉트론 학습 규칙
        
        $$
        w_{i,j}^{(next step)}=w_{i,j} + \eta(y_j - \hat y_j)x_i
        $$
        
        $w_{i,j}$: i번째 입력 뉴런과 j번째 출력 뉴런 사이를 연결하는 가중치 
        
        $x_i$: 현재 훈련 샘플의 i번째 뉴런의 입력값
        
        $\hat y_j$: 현재 훈련 샘플의 j번째 출력 뉴런의 출력값
        
        $y_j$: 현재 훈련 샘플의 j번째 출럭 뉴런의 타깃값
        
        $\eta$: 학습률
        
    - 각 출력 뉴런의 결정 경계는 선형이므로 복잡한 패턴을 학습하지 못 함
- 퍼셉트론 수렴 이론(Perceptron Convergence Theorem): 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 증명
- 퍼셉트론의 약점: 일부 간단한 문제를 풀 수 없음(다른 선형분류기도 마찬가지)
- 다층 퍼셉트론(MLP): 여러 개 쌓아올린 퍼셉트론, 일반 퍼셉트론보다 일부 제약이 줄음
- 퍼셉트론보다 로지스틱 회귀를 선호하는 이유: 퍼셉트론은…
    - 클래스 확률을 출력하지 않음
    - 정규화를 사용하지 않음
    - 훈련 세트에 더 이상 예측 오차가 없으면 즉시 훈련을 중단 → 일반화의 어려움
    
    하지만 퍼셉트론을 조금 더 빠르게 훈련 가능
    

### 다층 퍼셉트론과 역전파

- 다층 퍼셉트론의 구성
    - 입력 층: 1개
    - 은닉 층(Hidden layer): 하나 이상의 TLU 층
    - 출력 층
    - 하위 층(Lower layer): 입력 층과 가까운 층
    - 상위 층(Upper layer): 출력에 가까운 층
- 피드포워드 신경망(Feedforward Neural Network): 신호가 입력에서 출력으로 한 방향으로만 흐르는 신경망
- 심층 신경망(DNN: Deap Neural Network): 은닉 층을 여러 개 쌓아 올린 인공 신경망
    
    연산이 연속하여 길게 연결된 모델을 연구 
    
- 후진 모드 자동 미분(Reverse-mode Autrodiff, Reverse-mode Automatic Differeniation): 모든 그레이디언트를 자동으로 효율적으로 계산하는 기법
    
    네트워크를 두 번만 통과(전진, 후진)하면 모든 단일 모델 파라미터에 대한 신경망 오차의 그레이디언트를 계산
    
    신경망의 오차를 줄이기 위해 각 연결 가중치와 편향을 어떻게 조정해야 하는지 알아낼 수 있음
    
    미분 함수에 변수가 많고(연결 가중치 및 편향) 출력이 적을 때(하나의 손실) 적합
    
- 역전파(Backpropagation, Backprop): 후진 모드 미분과 경사 하강법을 결합
    
    그레이디언트를 자동으로 계산하고 경사 하강법 단계를 수행하는 과정을 반복하면 신경망의 오차가 점차 감소하여 결국 최솟값에 도달
    
    - 작동 방식
        
        정방향 계산(Forward Pass)
        
        - 한 번에 하나의 미니배치씩 진행하여 전체 훈련 세트를 처리(각 반복을 에포크)
        - 이 과정을 여러번 반복
        - 각 미니배치는 입력 층을 통해 네트워크로 들어감
            
            미니배치에 있는 모든 샘플에 대해 첫번째 은닉 층에 있는 모든 뉴런의 출력을 계산
            
        - 이 결과는 다음 층으로 전달되고 이 층의 출력을 계산
        - 이 과정을 마지막 층인 출력 층의 출력을 계산할 때까지 반복
        
        역방향 계산을 위해 중간 계산값을 모두 저장
        
        - 네트워크의 출력 오차를 측정
            
            손실 함수를 사용해 기대하는 출력과 네트워크의 실제 출력을 비교하고 오차 측정값을 반환
            
        - 각 출력의 오차와 출력 층의 각 연결이 이 오차에 얼마나 기여했는지 계산
            
            연쇄 법칙(Chain Rule)을 적용
            
        - 연쇄 법칙을 사용해 이전 층의 연결 가중치가 이 오차의 기여 정도에 얼마나 기여했는지 측정
        - 입력 층에 도달할 때까지 역방향으로 반복
            
            오차 그레이디언트를 거꾸로 전파함으로써 효율적으로 네트워크에 있는 모든 연결 가중치와 편향에 대한 오차 그레이디언트를 측정
            
        - 경사 하강법을 수행해 방금 계산한 오차 그레이디언트를 사용해 네트워크에 있는 모든 연결 가중치를 수정
        
        → 역전파 알고리즘이 먼저 미니배치에 대한 예측을 만들고(정방향 계산) 오차를 측정 
        → 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도를 측정(역방향 계산) 
        → 이 오차가 감소하도록 가중치와 편향을 조정(경사 하강법 단계)
        
    - 여러 활성화 함수
        - 로지스틱 함수(시그모이드 함수): $\sigma (z)=1/(1+ exp(-z))$
        - tanh 함수(하이퍼볼릭 탄젠트 함수): $tanh(z)=2\sigma (2z)-1$
            
            S자 모양이고 연속적이며 미분 가능
            
            출력 범위가 -1부터 1 사이라 훈련 초기에 각 층의 출력을 원점 근처로 모으는 경향이 존재(→ 빠르게 수렴하도록 도와줌)
            
        - ReLU 함수: $ReLU(z) =max(0,x)$
            
            연속적이지만 z=0에서 미분 불가능, z<0일 경우 도함수가 0
            
            잘 작동, 계산 속도가 빠르고 출력에 최댓값이 없음(경사 하강법의 일부 문제를 완화)
            
        
        비선형 활성화 함수가 있는 충분히 큰 심층 신경망은 이론적으로 어떤 연속함수도 근사 가능 (여러 개의 선형 변환은 선형 변환만 도출)
        

### 회귀를 위한 다층 퍼셉트론

다층 퍼셉트론은 회귀 작업에 사용 가능

- 출력 뉴련의 개수가 예측해야하는 값에 비례
- 사이킷런의 MLPRegressor 클래스로 MLP를 만들 수 있음
    - 평균 제곱 오차: 일반적으로 사용
    - 평균 절대 오차: 훈련 세트에 이상치가 많은 경우
    - 후버 손실(Huber loss, 평균 제곱 오차와 평균 절대 오차를 조합): 오차가 임계값 $\delta$(일반적으로 1)보다 작으면 이차 함수이고 크면 선형 함수
        
        선형 함수 부분은 평균 제곱 오차보다 이상치에 덜 민감
        
        이차 함수 부분은 평균 절대 오차보다 더 빠르고 정확하게 수렴하도록 도와줌
        
- MLP의 출력
    - 출력이 항상 양수임을 보장
        
        출력 층에 ReLU 활성화 함수를 사용하거나 ReLU의 부드러운 변형인 소프트플러스 함수(softplus(z) = log(1+exp(z)))를 사용
        
    - 출력이 항상 주어진 값의 범위 내에 속하도록 보장
        
        시그모이드 함수(0~1) 또는 쌍곡 탄젠트(-1~1)를 사용하고 적절한 번위로 타깃의 스케일을 조정
        

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/f360b679-d758-4a8e-8f6f-73c2250c366e/image.png)

### 분류를 위한 다층 퍼셉트론

분류에 사용되는 다층 퍼셉트론 

- 이진 분류 문제에서는 시그모이드 활성화 함수를 가진 하나의 출력 뉴런이 필요 → 출력을 양성 클래스에 대한 예측 확률로 해석
- 다중 레이블 이진 분류 문제 간단하게 처리: 출력 차원이 여러개(출력 층의 뉴런 개수)
    - 출력 층에 소프트맥스 활성화 함수 사용 → 모든 예측 확률을 0과 1 사이로 만듦, 클래스가 서로 배타적이기 때문에 모두 더하면 1
    - 확률 분포를 예측해야하므로 손실 함수에는 일반적으로 크로스 엔트로피 손실(cross-entropy loss, log loss)을 선택
    - 사이킷런의 MLPClasifier 클래스를 사용
        
        MSE가 아닌 크로스 엔트로피를 최소화 
        

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/560596fe-4130-4974-88e5-7c2165419221/image.png)

## 2. 케라스로 다층 퍼셉트론 구현하기

케라스: 모든 종류의 신경망을 손쉽게 만들고 훈련, 평가, 실행할 수 있는 텐서플로의 고수준 딥러닝 API

### 시퀀셜 API로 이미지 분류기 만들기

- 케라스로 데이터셋 적재하기: 케라스가 여러 데이터셋을 다운로드하고 적재할 수 있는 유틸리티 함수 제공
- 사이킷런과 케라스 데이터셋의 차이점
    
    각 이미지가 784 크기의 1D 배열이 아니라 28*28 크기의 배열
    
    픽셀 강도가 실수가 아니라 정수
    
- summary 메서드를 통해 모델의 이름, 출력 크기, 파라미터 개수 출력 가능
- 모델의 각 층의 이름
    - 생성자의 name 매개변수를 통해 직접 설정
    - 케라스가 자동으로 스네이크 표기법으로 변환
    
    → 모델 간에 이름을 고유하게 생성 (이름 충돌 없이 모델 병합 가능)
    
- 손실함수
    - sparse_categorical_crossentropy: 레이블이 정수 하나이고 클래스가 배타적
    - categorical_crossentropy: 샘플마다 클래스별 타깃 확률 존재
    - binary_crossentropy: 이진 분류나 다중 레이블 이진 분류 수행
- 출력 층
    - softmax: 출력이 배타적
    - sigmoid: 이진 분류나 다중 레이블 이진 분류 수행
- 옵티마이저
- 모델 훈련
    
    입력 특성, 타깃 클래스, 훈련할 에포크 횟수 전달 (+ 검증 세트)
    
    - 케라스는 에포크가 끝날 때마다 검증 세트를 사용해 손실과 추가적인 측정 지표 선택
    
    검증 세트 대신 케라스가 검증에 사용할 훈련 세트 비율 지정 가능
    
- 케라스에서 fit 메서드를 다시 호출하면 중지되었던 곳에서부터 훈련을 이어감

### 시퀀셜 API로 회귀용 다층 퍼셉트론 만들기

시퀀셜 API를 사용하여 회귀 MLP를 구축하고, 훈련하고, 평가하고, 사용하는 것은 분류 작업과 매우 비슷

- 가장 큰 차이점
    
    출력 층에서 하나의 값만 예측을 하고 싶기 때문에
    
    - 하나의 뉴런만 존재
    - 활성화 함수를 사용하지 않음
    - 손실 함수는 MSE
    - 측정 지표는 RMSE
    - 사이킷런의 MLPRegressor와 같이 Adam 옵티마이저 사용

→ 시퀀셜 API는 매우 깔끔하고 간단하지만 입력과 출력이 여러 개이거나 더 복잡한 네트워크 포톨로지를 갖는 신경망을 위해 함수형 API 사용

### 함수형 API로 복잡한 모델 만들기

- 와이드 & 딥 신경망 (Wide & Deep)
    
    입력의 일부 또는 전테가 출력 층에 바로 연결된 신경망
    
    → 복잡한 패턴(깊게 쌓은 층)과 간단한 규칙(짧은 경로)을 모두 학습 가능
    
- 이전과 동일하게 모델을 컴파일할 수 있지만 fit() 메서드를 호출할 때 하나의 입력 행렬을 전달하는 것이 아니라 입력마다 하나씩 행렬의 튜플을 전달 (evaluate나 predict 호출 시에도 동일)
- 여러 개의 출력이 필요한 경우
    - 동일한 데이터에서 독립적인 여러 작업을 수행할 때
    - 규제 기법으로 사용할 때: 과대적합을 줄이고 모델의 일반화 성능을 높이도록 훈련에 제약을 가함
        
        각 출력은 자신만의 손실 함수가 필요하므로 모델을 컴파일할 때 손실의 리스트를 전달
        
        기본적으로 케라스는 나열된 손실을 모두 더하여 최종 손실을 구해 훈련에 사용
        
        - 손실 가중치: 보조 출력보다 주 출력에 더 관심이 많을 경우 주 출력의 손실에 더 많은 가중치를 부여해야 함

### 서브클래싱 API로 동적 모델 만들기

시퀀셜 API와 함수형 API는 모두 선언적(declatative) → 사용할 층과 연결 방식을 먼저 정의해야하고 모델에 데이터를 주입하여 훈련이나 추론을 시작

서브클래싱 API는 명령형 프로그래밍 스타일로 여러 가지 동적 구조를 필요로 할 때 사용

- call() 메서드에 for 반복문, if 문, 저수준 텐서플로 연산 등 원하는 모든 것을 포함시킬 수 있음
    - 단점
        - 모델의 구조가 call() 메서드에 숨겨져 있어 케라스가 쉽게 검사 불가능
        - tf.keras.models.clone_model()을 사용해 모델 복제 불가능
        - summary() 메서드를 통해 층의 연결성을 확인할 수 없음
        - 케라스가 타입과 크기를 미리 확인할 수 없으므로 실수 가능

### 모델 저장과 복원하기

- save_format=”tf”
    
    텐서플로의 SavedModel 포맷을 사용해 모델을 저장
    
    - 저장한 이름의 디렉터리이며 여러 파일과 서브디렉터리를 포함
    - saved_model.pb 파일: 모델의 아키텍쳐와 로직이 직렬화된 계산 프로그램 형태로 포함되어 있음
    - keras_metadata.pb 파일: 케라스에 필요한 추가 정보 저장
    - variables 서브 디렉터리: 모든 파라미터 값
    - assets 디렉터리: 데이터 샘플, 특성 이름, 클래스 이름 등과 같은 추가 파일

- 모델을 로드
    - save_weights()와 load_weights()를 사용해 파라미터 값만 저장하고 로드 가능
        
        연결 가중치, 편향, 전처리 통계치, 옵티마이저 상태 등이 포함되어 있음
        
        파라미터 값은 한개 이상의 파일에 저장되며 my_weights.index와 같은 인덱스 파일이 존재
        
        → 가중치만 저장하는 방법이 상대적으로 빠르고 디스크 공간을 덜 사용하므로 체크포인트를 빠르게 저장하는데 적합
        

### 콜백 사용하기

큰 모델을 훈련하는데 매우 많은 시간이 필요하기 때문에 체크포인트를 정기적으로 저장해줘야 함

- fit() 메서드의 callbacks 매개변수를 사용해 케라스가 훈련의 시작 전이나 후에 호출할 객체 리스트를 저장 가능
- ModelCheckpoint: 훈련하는 동안 일정한 간격으로 모델의 체크포인트를 저장
    
    기본적으로 매 에포크의 끝에서 호출
    
    - “save_best_only=True”: 훈련하는 동안 검증 세트를 사용할 때 최상의 검증 세트 점수에서만 모델을 저장하도록 함
        
        검증 세트에서 최상의 점수를 낸 모델을 복원 가능
        
- EarlyStopping: 일정 에포크 동안 검증 세트에 대한 점수가 향상되지 않을 경우 훈련을 종료
    - “restore_best_weights=True”: 훈련이 끝난 후 최상의 모델을 복원
- 체크포인트 저장 콜백과 (과대적합을 막기 위해) 진전이 없는 경우 훈련을 일찍 멈추는 콜백을 함께 사용 가능
- 사용자 정의 콜백
    - 디버깅을 위해 사용자 예측 콜백을 검증과 예측 단계에서 사용 가능
    - 예측에도 사용가능

### 텐서보드로 시각화하기

인터랙티브 시각화 도구

훈련하는 동안 학습 곡선을 그림, 여러 실행 간의 학습 곡선을 비교하고 계산 그래프 시각화와 훈련 통계 분석, 모델이 생성한 이미지 확인, 3D에 투영된 다차원 데이터 시각화, 자동 클러스터링을 통해 네트워크 프로파일링 등을 수행

- 텐서보드 사용하기
    
    프로그램을 수정해 이벤트 파일(event file, 이진 로그 파일)에 시각화하려는 데이터를 출력해야 함
    
    summary: 각각의 이진 데이터 레코드
    
- 텐서보드 서버는 로그 디렉터리를 모니터링하고 자동으로 변경 사항을 읽어 그래프 업데이트
    
    → 실시간 데이터 시각화 가능
    
- 텐서보드 서버가 루트 로그 디렉터리를 가리키고 프로그램은 실행할 때마다 다른 서브디렉터리에 이벤트 기록
- TensorBoard() 콜백: 케라스가 제공하는 로그 디렉터리를 생성하고, 훈련 중에 이벤트 파일을 만들어 요약 정보를 기록하는 콜백
    
    모델의 훈련 및 검증 손실과 측정 지표를 계산
    
    신경망의 프로파일링 수행
    
    - 실행마다 하나의 디렉터리 생성
        
        그 아래 훈련 로그(프로파일링 트레이스 파일 포함)를 위한 서브디렉터리와 검증 로그를 위한 서브디렉터리가 포함
        
        → 둘 다 이벤트 파일을 담고 있음
        

## 3. 신경망 하이퍼파라미터 튜닝하기

신경망은 유연하기 때문에 조정할 하이퍼파라미터가 매우 많음

- 케라스 모델을 사이킷런 추정기로 변환해 GridSearchCV 또는 RandomizedSearchCV를 사용해 하이퍼파라미터 미세 튜닝
    
    SciKeras 라이브러리의 KerasRegressor와 KerasClassifier 사용 가능
    
- 케라스 튜너(Keras Tuner): 케라스 모델을 위한 하이퍼파라미터 튜닝 라이브러리 사용하기
    
    여러 가지 튜닝 전략 제공, 사용자 정의 가능, 텐서보드와의 통합이 뛰어남
    
    - RandomSearch 튜너
        1. 모든 하이퍼파라미터 사양을 수집하기 위해 빈 Hyperparameters 객체로 build_model() 한 번 호출
        2. max_trails 만큼 해당 범위 내에서 랜덤하게 샘플링된 하이퍼파라미터를 사용해 모델을 만든 다음 해당 모델을 epochs만큼 훈련
        3. directory의 서브 디렉터리에 저장
        4. overwrite를 통해 덮어쓰기 지정 가능
            
            “overwrite=False”일 경우, 이전 실행의 튜너가 중단한 지점부터 튜닝 실행
            
        5. objective를 통해 모델을 선호할 기준을 제시
    - 각 튜너는 오라클의 안내를 받음
        
        각 시도 전에 오라클에 다음 시도가 무엇인지를 요청
        
        오라클은 모든 시도를 기록하기 때문의 최상의 시도의 요약을 출력 가능