# 모델 훈련

- “모델을 훈련”: 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정

## 1. 선형 회귀

- 선형 회귀 모델의 예측
    
    가중치의 합과 편향(또는 절편)이라는 상수를 더해 예측을 생성
    
    $$
    \hat{y}= \theta_0 + \theta_1x_1+ \theta_2x_2+...+\theta_nx_n
    $$
    
    - 벡터 형태로 작성
        
        $$
        \hat{y}= h_\theta(x)=\theta *x
        $$
        
        머신러닝에선 종종 벡터를 하나의 열을 가진 열 벡터로 나타냄
        
- 모델을 훈련하기 위해 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정
    
    → 선형 회귀 모델을 훈련시키기 위해 RMSE를 최소화하는 $\theta$를 찾아야 함(MSE를 최소화하는 것이 같은 결과를 내며 더 간단)
    
    - 선형 회귀 모델의 MSE 비용 함수
        
        $$
        MSE(X,h_\theta) = (1/m)* \sum_{i=1}^m (\theta^Tx^{(i)}-y^{(i)})^2
        $$
        

### 정규 방정식

비용 함수를 최소화하는 $\theta$ 값을 찾기 위한 해석적인 방법

$$
\hat{\theta}=(X^TX)^{-1}X^Ty
$$

- $\hat{\theta}$: 비용 함수를 최소화하는 $\theta$ 값
- y는 y^1부터 y^m까지 포함하는 타깃 벡터
- 사이킷런에서 선형 회귀를 수행
    - 특성의 가중치(coef_)와 편향(intercept_)을 분리해 저장
    - scipy.linalg.lstsq() 함수를 기반으로 함
        
        → $X^+y$를 계산 ($X^+$는 X의 유사역행렬, 정규방정식보다 효율적인 방법)
        
        특수한경우( m<n이거나 어떤 특성의 중복으로 $X^TX$이 특이 행렬이라 역행렬이 존재하지 않는다면) 정규 방정식은 작동하지 않음
        

### 계산 복잡도

- (n+1)*(n+1) 크기의 $X^TX$의 역행렬을 계산하므로 계산복잡도는 일반적으로 $O(n^{2.4})$에서 $O(n^{3})$ 사이
- LinearRegression 클래스가 사용하는 SVD 방법은 약 $O(n^2)$

→ 정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려지지만 훈련 세트의 샘플 수에 대해서는 선형적으로 증가

- 학습된 선형 회귀 모델은 예측이 매우 빠르고 예측 계산 복잡도는 샘플 수와 특성 수에 선형적

## 2. 경사 하강법(gradient descent, GD)

여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘

- 기본 아이디어
    - 비용 함수를 최소화하기 위해 반복해서 파라미터 조정
    - 파리미터 벡터 $\theta$에 대해 비용 함수의 현재 그레이디언트(기울기)를 계산해 그레이디언트가 감소하는 방향으로 진행
        
        → 최솟값에 도달하면 그레이디언트가 0
        
    - $\theta$를 임의의 값으로 시작해서(랜덤 초기화) 한 번에 조금씩 비용 함수가 감소되는 방향으로 알고리즘이 최솟값에 수련할 때까지 진행
    - 학습 스텝의 크기는 비용 함수의 기울기에 비례하므로 최솟값에 가까워질수록 스텝 크기가 점진적으로 감소
- 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 그어도 곡선을 가로지르지 않는 볼록 함수 형태
    
    → 경사 하강법이 전역 최솟값에 가깝게 접근 보장
    
- 경사 하강법을 사용할 시 수렴할 때 훨씬 오래거리기 때문에 반드시 모든 특성의 스케일을 같게 만들어야 함
- 모델 훈련은 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일
    
    → 모델의 파라미터 공간에서 찾음 (모델이 가진 파라미터가 많을수록 차원이 커지고 검색이 어려워짐)
    

### 배치 경사 하강법

편도함수: 각 모델 파라미터 $\theta_j$에 대해 비용 함수의 기울기 계산 = $\theta_j$가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산 

- 비용 함수의 편도함수
    
    $$
    {\partial \over \partial \theta_j}MSE(\theta)={2\over m} \sum_{i=1}^m (\theta^Tx^{(i)}-y^{(i)})x_j^{(i)}
    $$
    
- 비용 함수의 그레이디언트 벡터
    
    $$
    \triangledown_\theta MSE(\theta)=\begin{bmatrix} {\partial \over \partial \theta_0}MSE(\theta) \\ {\partial \over \partial \theta_1}MSE(\theta) \\ ... \\ {\partial \over \partial \theta_n}MSE(\theta)\end{bmatrix} = {2 \over m}X^T(X\theta-y)
    $$
    
- 매 스텝에서 훈련 데이터 전체를 사용해 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산
    
    그렇기 때문에 매우 느리지만 특성 수에 민감하지 않아 자주 사용
    
- 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 가야 함
    
    $\theta- \triangledown_\theta MSE(\theta)$ 를 구해야 함
    
    - 내려가는 스텝의 크기를 결정하기 위해 그레이디언트 벡터에 $\eta$(학습률)을 곱함
- 적절한 학습률(스텝의 크기)를 찾기 위해 그리드 서치를 사용
    
    →적절한 반복 횟수를 찾기 위해 반복 횟수를 아주 크게 지정하고 그레이디언트 벡터가 아주 작아지면 (벡터의 노름이 허용 오차보다 작아지면) 알고리즘을 중지하는 방법을 사용
    
- 허용 오차에 반비례해 알고리즘의 반복횟수가 증가