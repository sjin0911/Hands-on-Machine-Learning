# 모델 훈련

- “모델을 훈련”: 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정

## 1. 선형 회귀

- 선형 회귀 모델의 예측
    
    가중치의 합과 편향(또는 절편)이라는 상수를 더해 예측을 생성
    
    $$
    \hat{y}= \theta_0 + \theta_1x_1+ \theta_2x_2+...+\theta_nx_n
    $$
    
    - 벡터 형태로 작성
        
        $$
        \hat{y}= h_\theta(x)=\theta *x
        $$
        
        머신러닝에선 종종 벡터를 하나의 열을 가진 열 벡터로 나타냄
        
- 모델을 훈련하기 위해 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정
    
    → 선형 회귀 모델을 훈련시키기 위해 RMSE를 최소화하는 $\theta$를 찾아야 함(MSE를 최소화하는 것이 같은 결과를 내며 더 간단)
    
    - 선형 회귀 모델의 MSE 비용 함수
        
        $$
        MSE(X,h_\theta) = {2 \over m} \sum_{i=1}^m (\theta^Tx^{(i)}-y^{(i)})^2
        $$
        

### 정규 방정식

비용 함수를 최소화하는 $\theta$ 값을 찾기 위한 해석적인 방법

$$
\hat{\theta}=(X^TX)^{-1}X^Ty
$$

- $\hat{\theta}$: 비용 함수를 최소화하는 $\theta$ 값
- y는 y^1부터 y^m까지 포함하는 타깃 벡터
- 사이킷런에서 선형 회귀를 수행
    - 특성의 가중치(coef_)와 편향(intercept_)을 분리해 저장
    - scipy.linalg.lstsq() 함수를 기반으로 함
 
        → $X^+y$를 계산 ($X^+$는 X의 유사역행렬, 정규방정식보다 효율적인 방법)
        
        특수한경우( m<n이거나 어떤 특성의 중복으로 $X^TX$이 특이 행렬이라 역행렬이 존재하지 않는다면) 정규 방정식은 작동하지 않음
        

### 계산 복잡도

- (n+1)*(n+1) 크기의 $X^TX$의 역행렬을 계산하므로 계산복잡도는 일반적으로 $O(n^{2.4})$에서 $O(n^{3})$ 사이
- LinearRegression 클래스가 사용하는 SVD 방법은 약 $O(n^2)$

→ 정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려지지만 훈련 세트의 샘플 수에 대해서는 선형적으로 증가

- 학습된 선형 회귀 모델은 예측이 매우 빠르고 예측 계산 복잡도는 샘플 수와 특성 수에 선형적

## 2. 경사 하강법(gradient descent, GD)

여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘

- 기본 아이디어
    - 비용 함수를 최소화하기 위해 반복해서 파라미터 조정
    - 파리미터 벡터 $\theta$에 대해 비용 함수의 현재 그레이디언트(기울기)를 계산해 그레이디언트가 감소하는 방향으로 진행
        
        → 최솟값에 도달하면 그레이디언트가 0
        
    - $\theta$를 임의의 값으로 시작해서(랜덤 초기화) 한 번에 조금씩 비용 함수가 감소되는 방향으로 알고리즘이 최솟값에 수련할 때까지 진행
    - 학습 스텝의 크기는 비용 함수의 기울기에 비례하므로 최솟값에 가까워질수록 스텝 크기가 점진적으로 감소
- 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 그어도 곡선을 가로지르지 않는 볼록 함수 형태
    
    → 경사 하강법이 전역 최솟값에 가깝게 접근 보장
    
- 경사 하강법을 사용할 시 수렴할 때 훨씬 오래거리기 때문에 반드시 모든 특성의 스케일을 같게 만들어야 함
- 모델 훈련은 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일
    
    → 모델의 파라미터 공간에서 찾음 (모델이 가진 파라미터가 많을수록 차원이 커지고 검색이 어려워짐)
    

### 배치 경사 하강법

편도함수: 각 모델 파라미터 $\theta_j$에 대해 비용 함수의 기울기 계산 = $\theta_j$가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산 

- 비용 함수의 편도함수
    
    $$
    {\partial \over \partial \theta_j}MSE(\theta)={2\over m} \sum_{i=1}^m (\theta^Tx^{(i)}-y^{(i)})x_j^{(i)}
    $$
    
- 비용 함수의 그레이디언트 벡터
    
    $$
    \triangledown_\theta MSE(\theta)=\begin{bmatrix} {\partial \over \partial \theta_0}MSE(\theta) \\ {\partial \over \partial \theta_1}MSE(\theta) \\ ... \\ {\partial \over \partial \theta_n}MSE(\theta)\end{bmatrix} = {2 \over m}X^T(X\theta-y)
    $$
    
- 매 스텝에서 훈련 데이터 전체를 사용해 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산
    
    그렇기 때문에 매우 느리지만 특성 수에 민감하지 않아 자주 사용
    
- 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 가야 함
    
    $\theta- \triangledown_\theta MSE(\theta)$ 를 구해야 함
    
    - 내려가는 스텝의 크기를 결정하기 위해 그레이디언트 벡터에 $\eta$(학습률)을 곱함
- 적절한 학습률(스텝의 크기)를 찾기 위해 그리드 서치를 사용
    
    →적절한 반복 횟수를 찾기 위해 반복 횟수를 아주 크게 지정하고 그레이디언트 벡터가 아주 작아지면 (벡터의 노름이 허용 오차보다 작아지면) 알고리즘을 중지하는 방법을 사용
    
- 허용 오차에 반비례해 알고리즘의 반복횟수가 증가

### 확률적 경사 하강법

매 스텝에서 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 그레이디언트를 계산하는 방법 

- 장단점
    - 한 번에 하나의 샘플에 대한 그레이디언트를 계산하기 때문에 매 스텝에서 전체 훈련 세트를 사용하는 배치 경사 하강법과 반대로 매우 빠름
    - 매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련 가능
    - 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치며 평균적으로 감소하므로 불안정하다고 할 수 있음
    - 비용 함수가 매우 불규칙한 경우, 알고리즘이 지역 최솟값을 건너뛰도록 도와줌 → 배치 경사 하강법보다 전역 최솟값을 찾을 가능성이 높음
    
    → 무작위성은 지역 최솟값에서 탈출시켜줘 좋지만 알고리즘을 전역 최솟값에 다다르지 못하게 함
    
- 무작위성의 딜레마를 해결하는 방법
    - 학습률을 점진적으로 감소(담금질 기법, Simulated annealing)
        - 학습 스케줄을 활용해 학습률을 점진적으로 감소시켜 전역 최솟값에 도달하도록 함(학습 스케줄: 매 반복에서 학습률을 결정하는 함수)
- 샘플을 랜덤으로 선택하기 때문에 어떤 샘플은 한 에포크에서 여러 번 선택될 수 있고 어떤 샘플은 전혀 선택되지 못할 수도 있음 → 성능 향상 가능한 방법이 더 없음
- 확률적 경사 하강법을 사용할 때 훈련 샘플이 독립 동일 분포를 만족해야 평균적으로 파라미터가 전역 최적점을 향한다고 봄

### 미니배치 경사 하강법

각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그레이디언트를 계산하는 것이 아니라 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산

- 주요 장점: 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 성능을 향상시킬 수 있음
- 미니배치를 어느 정도 크게 할 경우, SGD보다 덜 불규칙하게 움직이고 최솟값에 더 가까이 도달
- 지역 최솟값에서 빠져나오기는 더 힘들 수 있음

![선형 회귀를 사용한 알고리즘 비교](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/b15dc2ca-4e6a-4cdd-bc4b-4918573ce129/Untitled.png)

선형 회귀를 사용한 알고리즘 비교

- 알고리즘들간의 훈련 결과는 비슷

## 3. 다항 회귀

비선형 데이터를 학습하는 데 선형 모델 사용 가능 

- 각 특성의 거듭제곱을 새로운 특성으로 추가하고 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련
- 특성이 여러 개일 때 다항 회귀는 특성 사이의 관계 파악 가능
- PolynomialFeatures가 주어진 차수까지 특성 간의 모든 교차항을 추가

## 4. 학습 곡선

모델의 훈련 오차와 검증 오차를 훈련 반복 횟수의 함수로 나타낸 그래프

→ 훈련하는 동안 훈련 세트와 검증 세트에서 일정한 간격으로 모델을 평가하고 그 결과를 그래프로 그리기

- 모델의 과대, 과소 적합을 확인하는 방법
    
    고차 다항 회귀를 적용하면 일반 선형 회귀에서보다 훨씬 더 훈련 데이터에 잘 맞추려 노력 → 차수가 높아질수록 과대 적합될 가능성이 높아짐
    
    - 훈련 데이터에서 성능이 좋지만 교차 검증 점수가 나쁘다면 모델이 과대적합
    - 학습 곡선 확인
- 사이킷런의 교차 검증을 사용해 모델을 훈련하고 평가하는 learning_curve() 함수 사용
    
    훈련 세트의 크기를 증가시키면서 모델을 재훈련하지만 모델이 점진적 학습을 지원하는 경우 exploit_incremental_learning=True를 통해 모델을 점진적으로 훈련시킬 수 있음
    
    - 모델을 평가한 훈련 세트 크기를 반환하고 각각의 크기와 교차 검증 폴드에서 측정한  훈련 및 검증 점수 반환
- 학습 곡선의 형태
    - 과소적합인 경우: 두 곡선이 수평한 구간을 만들고 꽤 높은 오차에서 매우 가까이 근접
    - 과대적합인 경우: 두 곡선 사이의 공간이 있음

> **편향/분산 트레이드오프**
> 
> 
> 모델의 일반화 오차가 세 가지 다른 종류의 오차의 합으로 표현
> 
> - 편향: 잘못된 가정으로 인한 오류 → 편향이 클수록 데이터에 과소적합
> - 분산: 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감 → 자유도가 높을수록 분산이 높아 과대적합
> - 줄일 수 없는 오차: 데이터 자체에 있는 잡음때문에 발생하는 오차 → 오차를 줄이는 유일한 방법은 데이터 잡음 제거
> 
> 모델의 복잡도와 분산은 비례관계이고 복잡도와 편향은 반비례관계
> 

## 5. 규제가 있는 선형 모델

모델의 자유도를 줄여 데이터에 과대적합을 줄임

- 다항 회귀 모델일 경우 다항식의 차수를 줄임
- 선형 회귀 모델에서는 모델의 가중치를 제한

### 릿지 회귀 (ridge regression, tikhonov regularization)

규제가 추가된 선형 회귀 

- 규제항이 MSE에 추가
    
    $$
    규제항 = {\alpha \over m}\sum_{i=1}^n \theta_i^2
    $$
    
    - 학습 알고리즘을 데이터에 맞추고 모델의 가중치가 가능한 한 작게 유지
    - 훈련하는 동안에만 비용 함수에 추가
- 하이퍼파라미터 $\alpha$ : 모델을 얼마나 많이 규제할지 조절
    - 0일 경우 선형 회귀 모델과 같아지고 매우 클 경우 모든 가중치가 0에 가가워지M고 데이터의 평균을 지나는 수평선이 됨
- 릿지 회귀의 비용 함수
    
    $$
    J(\theta)=  MSE(\theta)+{\alpha \over m}\sum_{i=1}^n \theta_i^2
    $$
    
    - 편향 $\theta_0$ 는 규제되지 않음
    - $w$ 를 특성의 가중치 벡터($\theta_1$에서 $\theta_n$)라고 정의하면 규제항은 $\alpha(\lVert w \rVert_2)^2/m$ 과 같음
    - 배치 경사 하강법에 적용하려면 특성 가중치에 대한 MSE 그레이디언트 벡터에 $2\alpha w/m$을 더하면 됨
- 규제가 있는 모델 대부분은 입력 스케일에 민감하기 때문에 수행하기 전 데이터의 스케일을 StandardScaler를 통해 맞춰줘야 함
- 릿지 모델을 사용한 예측
    - $\alpha$가 증가할수록 직선에 가까워짐 → 모델의 분산은 줄지만 편향은 커짐
- 릿지 회귀를 계산하기 위해 사용하는 방법
    - 정규방정식을 사용하는 경우
        
        $$
        \hat\theta=(X^TX+\alpha A)^{-1}X^Ty
        $$
        
        A는 편향에 해당하는 맨 왼쪽 위의 원소가 0인 (n+1)*(n+1)의 단위 행렬
        
    - 경사 하강법을 사용하는 경우
        - SGDRegressor의 penalty 매개변수를 통해 사용할 규제를 지정
            
            “$l_2$ ”로 지정할 경우, MSE 비용 함수 가중치 벡터의 $l_2$  노름의 제곱에 alpha를 곱한 규제항이 추가 → m으로 나누는 부분을 제외한 릿지 회귀와 같음
            

### 라쏘 회귀

선형 회귀의 또 다른 규제된 버전

비용 함수에 규제항을 더하는 형식이지만 $l_2$  노름이 아닌 가중치 벡터의 $l_1$ 노름을 사용

$$
J(\theta)=  MSE(\theta)+2\alpha\sum_{i=1}^n {| \theta_i |}
$$

- 덜 중요한 특성의 가중치를 제거 → 자동으로 특성 선택을 수행하고 희소 모델을 만듦
- 라쏘의 비용 함수는 $\theta_i$=0에서 미분 불가능하지만  $\theta_i$=0일 때 서브그레이디언트 벡터 g를 사용하면 경사 하강법 적용 가능
    
    $$
    g(\theta,J)=\triangledown_\theta MSE(\theta)+\alpha\begin{pmatrix} sign(\theta_1)\\sign(\theta_2)\\...\\sign(\theta_n) \end{pmatrix}
    $$
    
    - 이 때, $\theta_i$는 음수일 때 -1, 0일 때 0, 양수일 때 1

### 엘라스틱넷

릿지 회귀와 라쏘 회귀를 절충한 모델 → 규제항은 릿지 회귀와 라쏘 회귀를 더한 것 

혼합 정도는 혼합 비율 r을 사용해 조절 

$$
J(\theta)=MSE(\theta)+r(2\alpha\sum_{i=1}^n|\theta_i|)+(1-r)({\alpha \over m}\sum_{i=1}^n\theta_i^2)
$$

### 조기 종료

검증 오차가 최솟값에 도달하면 훈련을 중지시키는 방식

- 2차 방정식 데이터셋 예시를 살펴보면 에포크가 진행됨에 따라 알고리즘의 예측 오차(RMSE)와 검증 세트에 대한 예측 오차가 점점 줄어들고 멈추었다가 다시 상승 → 훈련 데이터에 과대적합되기 시작
    - 확률적 경사 하강법이나 미니배치 경사 하강법에서는 곡선이 매끄럽지 않아 최솟값 도달 확인이 어려움 → 일정 시간 동안 검증오차가 최솟값보다 클 때 학습 중지하고 최소였을 때로 모델 파라미터를 되돌리는 방법 사용

## 6. 로지스틱 회귀

일부 회귀 알고리즘은 분류에서도 사용 가능

### 확률 추정

입력 특성의 가중치 합을 계산하지만 선형 회귀처럼 바로 결과를 출력하지 않고 결괏값의 로지스틱을 출력

$$
\hat p=h_\theta(x)=\sigma(\theta^Tx)
$$

- 로지스틱은 0과 1 사이의 값을 출력하는 시그모이드 함수
    
    $$
    \sigma(t)={1 \over 1+ep(-t)}
    $$
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/049a234c-be07-4cf6-a793-367009b2ae3f/Untitled.png)
    
- 로지스틱 회귀 모델이 샘플 x가 양성 클래스에 속할 확률($\hat p$ )을 추정하면 이에 대한 예측을 쉽게 구할 수 있음
    - $\hat p$ 이 0.5보다 작을 경우 0, 아닐 경우 1
- 기본 임곗값인 50% 확률을 사용하는 로지스틱 회귀 모델은 $\theta^Tx$가 양수일 때 1(양성 클래스)라고 예측하고, 음수일 때 0(음성 클래스)라고 예측

### 훈련과 비용 함수

로지스틱 회귀 모델 훈련의 목적은 양성 샘플에 대해서는 높은 확률을 추정하고 음성 샘플에 대해서는 낮은 확률을 추정하는 $\theta$ 를 찾는 것

- 하나의 훈련 샘플에 대한 비용 함수
    
    $$
    c(\theta)=
    \begin{cases}
    -log(\hat p), & \text{if y=1} \\
    -log(1-\hat p), & \text{if y=0}
    \end{cases}
    $$
    
- 로지스틱 회귀의 비용 함수(로그 손실)
    
    전체 훈련 세트에 대한 비용 함수는 모든 훈련 샘플의 비용을 평균한 것
    
    $$
    J(\theta)=-{1 \over m}\sum_{i=1}^m[y^{(i)}log(\hat p^{(i)})+(1-y^{(i)})log(1-\hat p^{(i)})]
    $$
    
    - 최솟값을 계산할 순 없지만 볼록 함수이므로 경사 하강법이 전역 최솟값을 보장

### 결정 경계

붓꽃 데이터셋을 사용 

- 붓꽃과 붓꽃 아님 양쪽의 확률이 똑같이 50%가 되는 1.6cm 근방에서 결정 경계가 만들어짐
- 결정 경계는 선형
- 로지스틱 회귀 모델도 $l_1$ , $l_2$ 패널티를 사용해 규제 가능하지만 사이킷런은 $l_2$ 패널티를 기본으로 사용

### 소프트맥스 회귀 (다항 로지스틱 회귀)

로지스틱 회귀 모델을 여러 개의 이진 분류기를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 일반화한 모델 

- 샘플 x가 주어지면 소프트맥스 회귀 모델이 각 클래스 k에 대한 점수 $s_k(x)$를 계산하고, 그 점수에 소프트맥스 함수를 적용하여 각 클래스의 확률을 추정
    
    $$
    s_k(x)=(\theta^{(k)})^Tx
    $$
    
    - 파라미터 벡터 $\theta^{(k)}$는 파라미터 행렬에 행으로 저장
- 소프트맥스 함수를 통과시켜 클래스 k에 속할 확률 $\hat p_k$를 추정 가능
    - 이 함수는 각 점수에 지수를 적용한 후 정규화
        
        $$
        \hat p_k=\sigma(s(x))_k={ \exp(s_k(x)) \over \sum_{j=1}^K \exp(s_j(x))}
        $$
        
        - K: 클래스 수
        - s(x)는 샘플 x에 대한 각 클래스의 점수를 담은 벡터
        - $\sigma(s(x))_k$는 샘플 x에 대한 각 클래스의 점수가 주어졌을 때 이 샘플이 클래스 k에 속할 추정 확률
- 로지스틱 회귀 분류기와 마찬가지로 기본적으로 추정 확률이 가장 높은 클래스를 선택
- 소프트맥스 회귀 분류기의 예측
    
    $$
    \hat y=argmax_k\sigma(s(x))_k=argmax_ks_k(x)=argmax((\theta^{(k)})^Tx)
    $$
    
    - argmax 연산은 함수를 최대화하는 변수의 값을 반환
- 소프트맥스 회귀 분류기는 한 번에 하나의 클래스만 예측 (다중 클래스이지만 다중 출력은 아님)
- 크로스 엔트로피(cross entropy) 비용 함수: 추정된 클래스의 확률이 타깃 클래스에 얼마나 잘 맞는 지 측정할 때 주로 사용
    
    최소화할수록 타깃 클래스에 대해 낮은 확률을 예측하는 모델을 ㅈ억제
    
    $$
    j(\Theta)=-{1 \over m}\sum_{i=1}^m\sum_{k=1}^K y_k^{(i)}log(\hat p_k^{(i)})
    
    $$
    
    - $y_k^{(i)}$는 i번째 샘플이 클래스 k에 속할 타깃 확률
    - 클래스가 두개만 존재할 경우 이 비용 함수는 로지스틱 회귀의 비용 함수와 같음
- 클래스 k에 대한 크로스 엔트로프의 그레이디언트 벡터
    
    $$
    \triangledown_{\theta^{(k)}}J(\Theta)={1 \over m}\sum_{i=1}^m(\hat p_k^{(i)}-y_k^{(i)})x^{(i)}
    $$
    
- 각 클래스에 대한 크레이디언트 벡터를 계산할 수 있으므로 비용 함수를 최소화하기 위한 파라미터 행렬 $\Theta$를 찾기 위해 경사 하강법 등 최적화 알고리즘 사용 가능