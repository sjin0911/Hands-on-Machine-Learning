# 모델 훈련

- “모델을 훈련”: 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정

## 1. 선형 회귀

- 선형 회귀 모델의 예측
    
    가중치의 합과 편향(또는 절편)이라는 상수를 더해 예측을 생성
    
    $$
    \hat{y}= \theta_0 + \theta_1x_1+ \theta_2x_2+...+\theta_nx_n
    $$
    
    - 벡터 형태로 작성
        
        $$
        \hat{y}= h_\theta(x)=\theta *x
        $$
        
        머신러닝에선 종종 벡터를 하나의 열을 가진 열 벡터로 나타냄
        
- 모델을 훈련하기 위해 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정
    
    → 선형 회귀 모델을 훈련시키기 위해 RMSE를 최소화하는 $\theta$를 찾아야 함(MSE를 최소화하는 것이 같은 결과를 내며 더 간단)
    
    - 선형 회귀 모델의 MSE 비용 함수
        
        $$
        MSE(X,h_\theta) = {2 \over m} \sum_{i=1}^m (\theta^Tx^{(i)}-y^{(i)})^2
        $$
        

### 정규 방정식

비용 함수를 최소화하는 $\theta$ 값을 찾기 위한 해석적인 방법

$$
\hat{\theta}=(X^TX)^{-1}X^Ty
$$

- $\hat{\theta}$: 비용 함수를 최소화하는 $\theta$ 값
- y는 y^1부터 y^m까지 포함하는 타깃 벡터
- 사이킷런에서 선형 회귀를 수행
    - 특성의 가중치(coef_)와 편향(intercept_)을 분리해 저장
    - scipy.linalg.lstsq() 함수를 기반으로 함
        
        → $X^+y$를 계산 ($X^+$는 X의 유사역행렬, 정규방정식보다 효율적인 방법)
        
        특수한경우( m<n이거나 어떤 특성의 중복으로 $X^TX$이 특이 행렬이라 역행렬이 존재하지 않는다면) 정규 방정식은 작동하지 않음
        

### 계산 복잡도

- (n+1)*(n+1) 크기의 $X^TX$의 역행렬을 계산하므로 계산복잡도는 일반적으로 $O(n^{2.4})$에서 $O(n^{3})$ 사이
- LinearRegression 클래스가 사용하는 SVD 방법은 약 $O(n^2)$

→ 정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려지지만 훈련 세트의 샘플 수에 대해서는 선형적으로 증가

- 학습된 선형 회귀 모델은 예측이 매우 빠르고 예측 계산 복잡도는 샘플 수와 특성 수에 선형적

## 2. 경사 하강법(gradient descent, GD)

여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘

- 기본 아이디어
    - 비용 함수를 최소화하기 위해 반복해서 파라미터 조정
    - 파리미터 벡터 $\theta$에 대해 비용 함수의 현재 그레이디언트(기울기)를 계산해 그레이디언트가 감소하는 방향으로 진행
        
        → 최솟값에 도달하면 그레이디언트가 0
        
    - $\theta$를 임의의 값으로 시작해서(랜덤 초기화) 한 번에 조금씩 비용 함수가 감소되는 방향으로 알고리즘이 최솟값에 수련할 때까지 진행
    - 학습 스텝의 크기는 비용 함수의 기울기에 비례하므로 최솟값에 가까워질수록 스텝 크기가 점진적으로 감소
- 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 그어도 곡선을 가로지르지 않는 볼록 함수 형태
    
    → 경사 하강법이 전역 최솟값에 가깝게 접근 보장
    
- 경사 하강법을 사용할 시 수렴할 때 훨씬 오래거리기 때문에 반드시 모든 특성의 스케일을 같게 만들어야 함
- 모델 훈련은 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일
    
    → 모델의 파라미터 공간에서 찾음 (모델이 가진 파라미터가 많을수록 차원이 커지고 검색이 어려워짐)
    

### 배치 경사 하강법

편도함수: 각 모델 파라미터 $\theta_j$에 대해 비용 함수의 기울기 계산 = $\theta_j$가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산 

- 비용 함수의 편도함수
    
    $$
    {\partial \over \partial \theta_j}MSE(\theta)={2\over m} \sum_{i=1}^m (\theta^Tx^{(i)}-y^{(i)})x_j^{(i)}
    $$
    
- 비용 함수의 그레이디언트 벡터
    
    $$
    \triangledown_\theta MSE(\theta)=\begin{bmatrix} {\partial \over \partial \theta_0}MSE(\theta) \\ {\partial \over \partial \theta_1}MSE(\theta) \\ ... \\ {\partial \over \partial \theta_n}MSE(\theta)\end{bmatrix} = {2 \over m}X^T(X\theta-y)
    $$
    
- 매 스텝에서 훈련 데이터 전체를 사용해 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산
    
    그렇기 때문에 매우 느리지만 특성 수에 민감하지 않아 자주 사용
    
- 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 가야 함
    
    $\theta- \triangledown_\theta MSE(\theta)$ 를 구해야 함
    
    - 내려가는 스텝의 크기를 결정하기 위해 그레이디언트 벡터에 $\eta$(학습률)을 곱함
- 적절한 학습률(스텝의 크기)를 찾기 위해 그리드 서치를 사용
    
    →적절한 반복 횟수를 찾기 위해 반복 횟수를 아주 크게 지정하고 그레이디언트 벡터가 아주 작아지면 (벡터의 노름이 허용 오차보다 작아지면) 알고리즘을 중지하는 방법을 사용
    
- 허용 오차에 반비례해 알고리즘의 반복횟수가 증가

### 확률적 경사 하강법

매 스텝에서 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 그레이디언트를 계산하는 방법 

- 장단점
    - 한 번에 하나의 샘플에 대한 그레이디언트를 계산하기 때문에 매 스텝에서 전체 훈련 세트를 사용하는 배치 경사 하강법과 반대로 매우 빠름
    - 매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련 가능
    - 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치며 평균적으로 감소하므로 불안정하다고 할 수 있음
    - 비용 함수가 매우 불규칙한 경우, 알고리즘이 지역 최솟값을 건너뛰도록 도와줌 → 배치 경사 하강법보다 전역 최솟값을 찾을 가능성이 높음
    
    → 무작위성은 지역 최솟값에서 탈출시켜줘 좋지만 알고리즘을 전역 최솟값에 다다르지 못하게 함
    
- 무작위성의 딜레마를 해결하는 방법
    - 학습률을 점진적으로 감소(담금질 기법, Simulated annealing)
        - 학습 스케줄을 활용해 학습률을 점진적으로 감소시켜 전역 최솟값에 도달하도록 함(학습 스케줄: 매 반복에서 학습률을 결정하는 함수)
- 샘플을 랜덤으로 선택하기 때문에 어떤 샘플은 한 에포크에서 여러 번 선택될 수 있고 어떤 샘플은 전혀 선택되지 못할 수도 있음 → 성능 향상 가능한 방법이 더 없음
- 확률적 경사 하강법을 사용할 때 훈련 샘플이 독립 동일 분포를 만족해야 평균적으로 파라미터가 전역 최적점을 향한다고 봄

### 미니배치 경사 하강법

각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그레이디언트를 계산하는 것이 아니라 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산

- 주요 장점: 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 성능을 향상시킬 수 있음
- 미니배치를 어느 정도 크게 할 경우, SGD보다 덜 불규칙하게 움직이고 최솟값에 더 가까이 도달
- 지역 최솟값에서 빠져나오기는 더 힘들 수 있음

![선형 회귀를 사용한 알고리즘 비교](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd252a3-b085-498c-82e5-d9b17ba77910/b15dc2ca-4e6a-4cdd-bc4b-4918573ce129/Untitled.png)

선형 회귀를 사용한 알고리즘 비교

- 알고리즘들간의 훈련 결과는 비슷

## 3. 다항 회귀

비선형 데이터를 학습하는 데 선형 모델 사용 가능 

- 각 특성의 거듭제곱을 새로운 특성으로 추가하고 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련
- 특성이 여러 개일 때 다항 회귀는 특성 사이의 관계 파악 가능
- PolynomialFeatures가 주어진 차수까지 특성 간의 모든 교차항을 추가

## 4. 학습 곡선

모델의 훈련 오차와 검증 오차를 훈련 반복 횟수의 함수로 나타낸 그래프

→ 훈련하는 동안 훈련 세트와 검증 세트에서 일정한 간격으로 모델을 평가하고 그 결과를 그래프로 그리기

- 모델의 과대, 과소 적합을 확인하는 방법
    
    고차 다항 회귀를 적용하면 일반 선형 회귀에서보다 훨씬 더 훈련 데이터에 잘 맞추려 노력 → 차수가 높아질수록 과대 적합될 가능성이 높아짐
    
    - 훈련 데이터에서 성능이 좋지만 교차 검증 점수가 나쁘다면 모델이 과대적합
    - 학습 곡선 확인
- 사이킷런의 교차 검증을 사용해 모델을 훈련하고 평가하는 learning_curve() 함수 사용
    
    훈련 세트의 크기를 증가시키면서 모델을 재훈련하지만 모델이 점진적 학습을 지원하는 경우 exploit_incremental_learning=True를 통해 모델을 점진적으로 훈련시킬 수 있음
    
    - 모델을 평가한 훈련 세트 크기를 반환하고 각각의 크기와 교차 검증 폴드에서 측정한  훈련 및 검증 점수 반환
- 학습 곡선의 형태
    - 과소적합인 경우: 두 곡선이 수평한 구간을 만들고 꽤 높은 오차에서 매우 가까이 근접
    - 과대적합인 경우: 두 곡선 사이의 공간이 있음

> **편향/분산 트레이드오프**
> 
> 
> 모델의 일반화 오차가 세 가지 다른 종류의 오차의 합으로 표현
> 
> - 편향: 잘못된 가정으로 인한 오류 → 편향이 클수록 데이터에 과소적합
> - 분산: 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감 → 자유도가 높을수록 분산이 높아 과대적합
> - 줄일 수 없는 오차: 데이터 자체에 있는 잡음때문에 발생하는 오차 → 오차를 줄이는 유일한 방법은 데이터 잡음 제거
> 
> 모델의 복잡도와 분산은 비례관계이고 복잡도와 편향은 반비례관계
> 

## 5. 규제가 있는 선형 모델

모델의 자유도를 줄여 데이터에 과대적합을 줄임

- 다항 회귀 모델일 경우 다항식의 차수를 줄임
- 선형 회귀 모델에서는 모델의 가중치를 제한

### 릿지 회귀 (ridge regression, tikhonov regularization)

규제가 추가된 선형 회귀 

- 규제항이 MSE에 추가
    
    $$
    규졔항 = {\alpha \over m}\sum_{i=1}^n \theta_i^2
    $$
    
    - 학습 알고리즘을 데이터에 맞추고 모델의 가중치가 가능한 한 작게 유지
    - 훈련하는 동안에만 비용 함수에 추가
- 하이퍼파라미터 $\alpha$ : 모델을 얼마나 많이 규제할지 조절
    - 0일 경우 선형 회귀 모델과 같아지고 매우 클 경우 모든 가중치가 0에 가가워지M고 데이터의 평균을 지나는 수평선이 됨
- 릿지 회귀의 비용 함수
    
    $$
    J(\theta)=  MSE(\theta)+{\alpha \over m}\sum_{i=1}^n \theta_i^2
    $$
    
    - 편향 $\theta_0$ 는 규제되지 않음
    - $w$ 를 특성의 가중치 벡터($\theta_1$에서 $\theta_n$)라고 정의하면 규제항은 $\alpha(\lVert w \rVert_2)^2/m$ 과 같음